{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FKL_ae1XBjsZ"
      },
      "source": [
        "# Assignment 3: Korean to English Translation\n",
        "\n",
        "- Sequence to Sequence 모델의 대표적인 한국어-영어 번역을 Encoder-decoder, Attention, Convolution, 그리고 Transformers 기반으로 구현\n",
        "- Pytorch Seq to Seq 모델 (https://github.com/bentrevett/pytorch-seq2seq) 참고로 하여 한국어와 영어의 형태소분석되고 의존관계로 되어 있는 파일을 프로세싱하여 두 언어의 parallel 데이터 쌍으로 만들고 이를 학습하여 모델별로 Perplexity가 어떻게 달라지는지 살펴 보고, 가장 성능이 좋은 모델을 근간으로 해서 Inference로 한국어 문장을 입력하면 대응되는 영어 번역이 출력될 수 있도록 구현\n",
        "- 반드시 다음 세 모델에 대해서 PPL와 BLEU score가  다 체크되어야 함. **(Packed) Encoder-Decoder, Convolutional Seq to Seq, Transformers.\n",
        "- **새로운 버전의 TorchText를 사용하여 코랩에서 실행가능하도록**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvZxrA2Bjsc"
      },
      "source": [
        "## Data\n",
        "- 첨부된 ko-en-en.parse.syn은 330,974 한국어 문장에 대응되는 영어문장이 품사와 구문분석이 되어 있는 파일이고 ko-en-ko.parse.syn은 이에 대응되는 한국어 문장이 형태소와 구문분석이 되어 있는 파일이다.\n",
        "\n",
        "(ROOT (S (NP (NNP Flight) (NNP 007)) (VP (MD will) (VP (VB stay) (PP (IN on) (NP (NP (DT the) (NN ground)) (PP (IN for) (NP (CD one) (NN hour))))))) (. .)))\n",
        "\n",
        "\n",
        "<id 1>\n",
        "<sent 1>\n",
        "1       2       NP      777/SN\n",
        "2       6       NP_SBJ  항공편/NNG|은/JX\n",
        "3       4       NP      1/SN|시간/NNG\n",
        "4       6       NP_AJT  동안/NNG\n",
        "5       6       NP_AJT  지상/NNG|에/JKB\n",
        "6       7       VP      머물/VV|게/EC\n",
        "7       0       VP      되/VV|ㅂ니다/EF|./SF\n",
        "</sent>\n",
        "</id>\n",
        "\n",
        "- 이 두 파일을 프로세싱하여 한-영 병행 데이터로 만들고 이를 학습 및 테스트 데이터로 사용한다.\n",
        "- Hint: 구조화된 데이터를 프로세싱하기 위해서는 nltk의 모듈을 사용할 수 있다.\n",
        "\n",
        "- 한국어 형태소 분석된 단위를 어절별로 결합할 수 있고, 분석된 채로 그대로 사용할 수도 있다.\n",
        "- 두 언어의 어순을 비슷하게 데이터를 만들어 학습할 수도 있고, 번역의 성능을 높이기 위해 다양한 형태로 재구조화 할 수 있다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JUXVD-LU1nze"
      },
      "source": [
        "# Packed Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx16lxHgrgM6",
        "outputId": "b0eb9909-072c-449b-f94d-cc34d31d44f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "import portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsY2gpD0urg0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import torch \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HdiLWhT0uuTH",
        "outputId": "3106875f-f48c-4ebc-833d-7b128292d01b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.15.2+cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r368nrYu00q"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqvBp-bBrdo"
      },
      "source": [
        "### 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E5zoJMABoJw",
        "outputId": "2798540e-f931-4019-eb50-55b266441627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkuHShw1ERlm"
      },
      "source": [
        "### 파일 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj-5lkYSEYkA"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJOxyp9dHEhi"
      },
      "outputs": [],
      "source": [
        "ko_path = PATH+\"ko-en.ko.parse\"\n",
        "en_path = PATH+\"ko-en.en.parse.syn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j_dlxhBEhc1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaDCByI1YXGB"
      },
      "outputs": [],
      "source": [
        "ko_lines = \"\"\n",
        "with open(ko_path, \"r\", encoding='utf-8') as ko_file:\n",
        "    for line in ko_file.readlines():\n",
        "        ko_lines += line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mng6pPAvFUFs"
      },
      "outputs": [],
      "source": [
        "with open(en_path, \"r\", encoding='utf-8') as en_file:\n",
        "    en_lines = en_file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzwE9-lAj4Vd"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0t5cOS4JeWL"
      },
      "outputs": [],
      "source": [
        "from nltk import Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6DCZZ9WT-lf"
      },
      "outputs": [],
      "source": [
        "# nltk의 Tree 모듈을 사용하여 필요한 정보 추출, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "full_en_text_list = []\n",
        "for line in en_lines:\n",
        "    sent = '<sos>' + ' '\n",
        "    t = Tree.fromstring(line)\n",
        "    for token in t.leaves():\n",
        "      sent += token + ' '\n",
        "    sent += '<eos>'\n",
        "    full_en_text_list.append(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNULprznNwp0",
        "outputId": "870aceb5-b37e-497a-e88a-01bac457ce97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> Flight 007 will stay on the ground for one hour . <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>',\n",
              " \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\",\n",
              " '<sos> The official exchange rate is around 1,250 Won . <eos>',\n",
              " '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>',\n",
              " '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>',\n",
              " '<sos> Do you have change for $ 100 ? <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " \"<sos> I 'd like to change $ 100 . <eos>\",\n",
              " '<sos> Change 100 dollars . <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " '<sos> One hundred dollars . <eos>',\n",
              " \"<sos> I want four 100 's , two 20 's , five 10 's and ten 1 's . <eos>\",\n",
              " '<sos> 6 ten dollar bills and 8 five dollar bills , please . <eos>',\n",
              " '<sos> Could I have change for a one-hundred dollar bill ? <eos>',\n",
              " \"<sos> I 'd like to change one hundred . <eos>\",\n",
              " '<sos> I want four hundreds , three twenties , three tens , one five , and five ones . <eos>',\n",
              " '<sos> 100 miles is credited to your account . <eos>',\n",
              " '<sos> Five tens , and ten twenties , please . <eos>',\n",
              " '<sos> About 10 dollars . <eos>',\n",
              " '<sos> I want seven ten dollar bills and thirty one dollar bills . <eos>',\n",
              " '<sos> Two ten dollar bills , and two five dollar bills please . <eos>',\n",
              " '<sos> Make it 30 ten-dollar bills and small change . <eos>',\n",
              " '<sos> 8 ten-dollar bills and 4 five dollar bills , please . <eos>',\n",
              " '<sos> Fifty 10 dollar bills , please . <eos>',\n",
              " '<sos> Can I have change for this 10 dollar bill ? <eos>',\n",
              " '<sos> Could you give me small change for a ten dollar bill ? <eos>',\n",
              " '<sos> I will pay $ 10 in cash and the rest in credit card . <eos>',\n",
              " '<sos> 10dollars . <eos>',\n",
              " '<sos> Ten ten-dollar bills and ten one-dollar bills , please . <eos>',\n",
              " '<sos> Ten tens , ten fives , and fifty ones , please . <eos>',\n",
              " '<sos> 10 ten-dollar bills and 100 one-dollar bills , please . <eos>',\n",
              " '<sos> Twenty 10 dollar bills , forty 5 dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 's , two 5 's and ten 1 's . <eos>\",\n",
              " '<sos> Four tens , four fives and the rest in singles , please . <eos>',\n",
              " '<sos> 4 tens , and the rest in one dollar bills . <eos>',\n",
              " '<sos> 7 ten dollar bills and 6 five dollar bills , please . <eos>',\n",
              " '<sos> 2 tens and the rest in one dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 dollar bills , and the rest in 1 dollar bills . <eos>\",\n",
              " '<sos> Ten tens , five twenties and coins for the rest , please . <eos>',\n",
              " \"<sos> I 'd like 10 ten-dollar bills , 20 five-dollar bills and 30 one-dollar bills . <eos>\",\n",
              " '<sos> 10dollar bills , please . <eos>',\n",
              " '<sos> Could you give me ten one-dollar bill for this ten dollar bill ? <eos>',\n",
              " '<sos> Thirty ten-dollar bills and small change for ten dollars . <eos>',\n",
              " '<sos> Please change this for ten dollar bills . <eos>',\n",
              " '<sos> Ten dollar bills , please . <eos>',\n",
              " '<sos> Would you like tens or twenties ? <eos>',\n",
              " '<sos> In tens and twenties . <eos>',\n",
              " '<sos> 5 ten dollar bills , 15 five-dollar bills and 20 singles . <eos>',\n",
              " '<sos> I need six tens and small change for the rest . <eos>',\n",
              " \"<sos> I 'd like to have 9 tens and the rest in fives . <eos>\",\n",
              " '<sos> Could you change this 10 dollar bill for me ? <eos>',\n",
              " \"<sos> I 'd like to have some tens and twenties . <eos>\",\n",
              " '<sos> In 10 and 20 dollar bills . <eos>',\n",
              " '<sos> Do you like it in dimes ? <eos>',\n",
              " '<sos> This is 10 kilograms overweight . <eos>',\n",
              " '<sos> This is 12 kilograms overweight . <eos>',\n",
              " '<sos> There are 10 kinds of them . <eos>',\n",
              " '<sos> To upgrade to the diamond level requires 100,000 points or more . <eos>',\n",
              " \"<sos> It 's Gate 10 . <eos>\",\n",
              " '<sos> Could you tell me the way to Gate 10 ? <eos>',\n",
              " '<sos> Please proceed to Gate 10 . <eos>',\n",
              " '<sos> It will depart in ten minutes . <eos>',\n",
              " '<sos> 10 minutes . Never mind . <eos>',\n",
              " '<sos> October 11th . <eos>',\n",
              " '<sos> We reserved a seat for October 25th , eleven a.m. <eos>',\n",
              " \"<sos> I 'd like to change the date to October 9th . <eos>\",\n",
              " \"<sos> I 'll be here for ten days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> For about ten days . <eos>',\n",
              " \"<sos> I 'll stay for 10 days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> 10 days . <eos>',\n",
              " '<sos> For ten days . <eos>',\n",
              " '<sos> We have 10 different brands . <eos>',\n",
              " '<sos> Where is Gate 11 ? <eos>',\n",
              " '<sos> Would you tell me how to get to Gate 11 ? <eos>',\n",
              " '<sos> It departs at 11:00 . <eos>',\n",
              " '<sos> How about the 11th ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> 12G is over there on the right side . <eos>',\n",
              " '<sos> Please go to Gate 12 . <eos>',\n",
              " '<sos> Please come to Gate 12 . <eos>',\n",
              " \"<sos> It 's time for boarding now at Gate 12 . <eos>\",\n",
              " '<sos> Boarding begins at 12:10 . <eos>',\n",
              " '<sos> The arrival time is twelve thirty local time . <eos>',\n",
              " \"<sos> Let me have two seats on the 12 o'clock flight . <eos>\",\n",
              " '<sos> I want to cancel my December 25 flight . <eos>',\n",
              " '<sos> I want to make it on December 27 . <eos>',\n",
              " '<sos> Please cancel my flight for December 4th . <eos>',\n",
              " \"<sos> I 'd like to change the date to December 7th . <eos>\",\n",
              " '<sos> Go to gate 13 , please . <eos>',\n",
              " \"<sos> There 's 13 hour 's difference . <eos>\",\n",
              " '<sos> One hundred forty dollars . Which seat do you prefer ? <eos>',\n",
              " \"<sos> There 's a 14 hour 's difference between two cities . Seoul is 14 hours ahead . <eos>\",\n",
              " '<sos> 15dollars , please . <eos>',\n",
              " '<sos> Where is seat 15A ? <eos>',\n",
              " '<sos> Your flight will depart from gate 15 . <eos>',\n",
              " \"<sos> There 's a 15 hour 's difference . Seoul is 14 hours ahead . <eos>\"]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_en_text_list[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLQxlgtvp-WU",
        "outputId": "6d9c6777-eed3-482c-f6c5-31bc018ab92d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<id 1>',\n",
              " '<sent 1>',\n",
              " '1\\t2\\tNP\\t777/SN',\n",
              " '2\\t6\\tNP_SBJ\\t항공편/NNG|은/JX',\n",
              " '3\\t4\\tNP\\t1/SN|시간/NNG',\n",
              " '4\\t6\\tNP_AJT\\t동안/NNG',\n",
              " '5\\t6\\tNP_AJT\\t지상/NNG|에/JKB',\n",
              " '6\\t7\\tVP\\t머물/VV|게/EC',\n",
              " '7\\t0\\tVP\\t되/VV|ㅂ니다/EF|./SF',\n",
              " '</sent>',\n",
              " '']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 한 문장씩 나눈 리스트 만들기\n",
        "ko_list = ko_lines.split(\"</id>\")\n",
        "for i in range(len(ko_list)):\n",
        "    ko_list[i] = ko_list[i].split(\"\\n\")\n",
        "\n",
        "ko_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsKiqHtnRJvT"
      },
      "outputs": [],
      "source": [
        "# 필요한 정보만 추출\n",
        "import re\n",
        "\n",
        "pattern = r\"[가-힣ㄱ-ㅎ]+|[0-9]+(?=\\/SN)\"\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    for j in range(len(ko_list[i])):\n",
        "        ko_list[i][j] = re.findall(pattern, ko_list[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sigvTdgxtnyA",
        "outputId": "eb88af08-2d2f-440e-d3e7-933105722402"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[[],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['1', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['머물', '게'],\n",
              "  ['되', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['3', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['있', '겠', '습니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['1', '000', '달러'],\n",
              "  ['여행자', '수표', '가'],\n",
              "  ['필요', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['1', '250', '원', '이'], ['공식'], ['환율', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['3', '장', '과'],\n",
              "  ['나머지', '는'],\n",
              "  ['20', '달러', '권', '으로'],\n",
              "  ['주', '시', 'ㅂ시오'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['한'],\n",
              "  ['장', '과'],\n",
              "  ['50', '달러'],\n",
              "  ['4', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '를'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '만'], ['바꾸', '어'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전'],\n",
              "  ['좀'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['어', '주', '어', '요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['6', '개'],\n",
              "  ['하', '고'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['8', '개', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '겠', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '불', '을'], ['바꾸', '겠', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '불', '짜리'],\n",
              "  ['4', '매'],\n",
              "  ['20', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['1', '매'],\n",
              "  ['그리고'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['5', '매'],\n",
              "  ['원하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100'], ['마일리지', '가'], ['적립', '되', '었', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['5', '장', '과'],\n",
              "  ['20', '달러'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['부탁', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러'], ['정도', '이', '에요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['7', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['서른'],\n",
              "  ['장', '과'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['여덟'],\n",
              "  ['장하', '고'],\n",
              "  ['5', '달러'],\n",
              "  ['지폐'],\n",
              "  ['네'],\n",
              "  ['장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '로'],\n",
              "  ['50', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ', '려고요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '는'],\n",
              "  ['현금', '으로'],\n",
              "  ['하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['카드', '로'],\n",
              "  ['계산', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['그리고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['50', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장', '하', '고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['100', '장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['20', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['40', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['3', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장', '하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['7', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['6', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['세'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '과'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['다섯'],\n",
              "  ['장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['동전', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['20', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리'], ['지폐', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '가'],\n",
              "  ['서른'],\n",
              "  ['10', '달러', '는'],\n",
              "  ['잔돈', '이', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  ['아니', '면'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '와'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['15', '장'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['20', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['6', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []]]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_list[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwfGmvdDUESX"
      },
      "outputs": [],
      "source": [
        "# 문장으로 만들어서 리스트에 넣어주기, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "ko_text_list = []\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    full_sent = '<sos>' + ' '\n",
        "    for j in range(len(ko_list[i])):\n",
        "        if ko_list[i][j]:\n",
        "            for token in ko_list[i][j]:\n",
        "                full_sent += token + ' '\n",
        "    full_sent += '<eos>'\n",
        "\n",
        "    ko_text_list.append(full_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AShBC5GRVXF",
        "outputId": "f5e49b6c-2bb2-442f-8a45-5489bc69db65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>',\n",
              " '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>',\n",
              " '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 100 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 짜리 4 장 20 달러 짜리 2 장 10 달러 짜리 5 장 1 달러 짜리 10 장 으로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 6 개 하 고 5 달러 짜리 지폐 8 개 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 겠 습니까 <eos>',\n",
              " '<sos> 100 불 을 바꾸 겠 습니다 <eos>',\n",
              " '<sos> 100 불 짜리 4 매 20 불 짜리 3 매 10 불 짜리 3 매 5 불 짜리 1 매 그리고 1 불 짜리 5 매 원하 ㅂ니다 <eos>',\n",
              " '<sos> 100 마일리지 가 적립 되 었 습니다 <eos>',\n",
              " '<sos> 10 달러 5 장 과 20 달러 10 장 으로 부탁 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 정도 이 에요 <eos>',\n",
              " '<sos> 10 달러 지폐 7 장 1 달러 지폐 30 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 두 장 5 달러 짜리 두 장 주 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 서른 장 과 잔돈 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 여덟 장하 고 5 달러 지폐 네 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 로 50 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 습니까 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 려고요 <eos>',\n",
              " '<sos> 10 달러 는 현금 으로 하 고 나머지 는 카드 로 계산 하 아 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 5 달러 짜리 10 장 그리고 1 달러 짜리 50 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 하 고 1 달러 짜리 100 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 20 장 5 달러 짜리 40 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 3 장 5 달러 짜리 2 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 5 달러 짜리 4 장 그리고 나머지 는 1 달러 짜리 로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 하 고 나머지 는 1 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 7 장 5 달러 짜리 6 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 두 장이 랑 나머지 는 1 달러 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 세 장이 랑 나머지 는 1 달러 로 주 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 열 장 과 20 달러 짜리 다섯 장 그리고 나머지 는 동전 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 10 장 5 달러 짜리 지폐 20 장 1 달러 지폐 30 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 를 1 달러 짜리 열 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 가 서른 10 달러 는 잔돈 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 하 ㄹ까요 아니 면 20 달러 짜리 로 하 ㄹ까요 <eos>',\n",
              " '<sos> 10 달러 짜리 와 20 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 5 장 5 불 짜리 15 장 1 불 짜리 20 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 6 장 그리고 나머지 는 잔돈 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 9 장 나머지 는 5 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 10 불 짜리 와 20 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 하 고 20 불 짜리로요 <eos>',\n",
              " '<sos> 10 센트 로 드리 ㄹ 까요 <eos>',\n",
              " '<sos> 10 무게 초과 되 시 었 습니다 <eos>',\n",
              " '<sos> 12 초과 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 가지 종류 가 있 는데요 <eos>',\n",
              " '<sos> 10 만 점 이상 부터 다이아몬드 등급 으로 상향 조정 되 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 게이트 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 출구 로 가 는 길 을 알리 러 주 시 겠 습니까 <eos>',\n",
              " '<sos> 10 번 출구 로 가 시 어요 <eos>',\n",
              " '<sos> 10 분 후 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 분 이 ㅂ니다 신경 쓰 지 말 시 어요 <eos>',\n",
              " '<sos> 10 월 11 일 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 월 25 일 오전 11 시 발 비행 편 에 예약 되 었 습니다 <eos>',\n",
              " '<sos> 10 월 9 일 로 변경 하 고 싶 은데요 <eos>',\n",
              " '<sos> 10 일 동안 있 을 거 이 에요 <eos>',\n",
              " '<sos> 열흘 동안 이 요 <eos>',\n",
              " '<sos> 십 일 정도 <eos>',\n",
              " '<sos> 열흘 간 머무르 ㄹ 려구요 <eos>',\n",
              " '<sos> 십 일 간 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 종류 의 상표 가 있 어요 <eos>',\n",
              " '<sos> 11 번 게이트 는 어디 에 있 나요 <eos>',\n",
              " '<sos> 11 번 게이트 에 어떻 게 가 는지 알리 러 주 시 겠 어요 <eos>',\n",
              " '<sos> 11 시 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 11 일 은 어떻 어요 <eos>',\n",
              " '<sos> 12 자리 가 어디 죠 <eos>',\n",
              " '<sos> 12 좌석 은 어디 이 ㅂ니까 <eos>',\n",
              " '<sos> 12 좌석 은 저기 오른쪽 에 있 습니다 <eos>',\n",
              " '<sos> 12 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 12 번 게이트 로 오 시 어요 <eos>',\n",
              " '<sos> 12 번 창구 에서 지금 탑승 하 시 ㅂ시오 <eos>',\n",
              " '<sos> 12 시 10 분 에 탑승 을 시작 하 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 30 분 에 도착 예정 이 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 편 두 장 주 시 어요 <eos>',\n",
              " '<sos> 12 월 25 일 편 을 취소 하 고 싶 은데요 <eos>',\n",
              " '<sos> 12 월 27 일 로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 12 월 4 일 예약 취소 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 12 월 7 일 로 날짜 를 바꾸 고 싶 은데요 <eos>',\n",
              " '<sos> 13 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 13 시간 의 시차 가 있 습니다 <eos>',\n",
              " '<sos> 140 달러 이 ㅂ니다 어느 쪽 좌석 을 원하 시 어요 <eos>',\n",
              " '<sos> 14 시간 의 시차 가 있 습니다 서울 이 14 시간 빠르 죠 <eos>',\n",
              " '<sos> 15 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 15 좌석 은 어디 이 에요 <eos>',\n",
              " '<sos> 15 번 탑승구 는 번 중앙 홀 에 있 습니다 <eos>',\n",
              " '<sos> 15 시간 이 ㅂ니다 서울 이 14 시간 빠릅 니다 <eos>']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_text_list[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RnISPkkMNIn"
      },
      "source": [
        "### 한-영 병행 데이터 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAcTlAu-MPdH",
        "outputId": "62f94494-b652-4dab-b4d2-ce7c872279d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<sos> Flight 007 will stay on the ground for one hour . <eos>', '<sos> Flight 017 will stay on the ground for three hours . <eos>', \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\", '<sos> The official exchange rate is around 1,250 Won . <eos>', '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>', '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>', '<sos> Do you have change for $ 100 ? <eos>', \"<sos> I 'd like to change 100 dollars . <eos>\", \"<sos> I 'd like to change $ 100 . <eos>\", '<sos> Change 100 dollars . <eos>']\n",
            "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>', '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>', '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>', '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>', '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>', '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>', '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>', '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>']\n"
          ]
        }
      ],
      "source": [
        "print(full_en_text_list[:10])\n",
        "print(ko_text_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Mc2t41gTcE"
      },
      "outputs": [],
      "source": [
        "parallel_data = list(zip(ko_text_list, full_en_text_list)) # src:한국어, trg:영어 순으로 넣어줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgH7krRygahK",
        "outputId": "b2c702bd-3397-4760-c8fd-bcb6f97d80e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> Flight 007 will stay on the ground for one hour . <eos>')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMHnfWNMlf1u",
        "outputId": "ad9e12f3-c29f-4b22-df1b-9a3726b5d92a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_1M4a_ltGO7",
        "outputId": "a3dc1a4d-df0c-405e-9819-62a1c100ee68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "330974"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parallel_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pzVWVqbBcBs"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXA2QqN_v7cO"
      },
      "outputs": [],
      "source": [
        "# 전체 데이터 사용 시 런타임 에러가 발생하여 100000개의 데이터만 사용\n",
        "train_iter = parallel_data[0:70000]\n",
        "valid_iter = parallel_data[70000:90000]\n",
        "test_iter = parallel_data[90000:100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T6tmwK5t9gd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "valid_dataset = to_map_style_dataset(valid_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3JE3KcnuJzz"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(file_iter, type = 0):\n",
        "     \n",
        "     if type == 0:\n",
        "        for line in file_iter:\n",
        "            yield line[0].split()\n",
        "     else:\n",
        "        for line in file_iter:\n",
        "            yield line[1].split()\n",
        "\n",
        "src_voc = build_vocab_from_iterator(yield_tokens(train_iter, 0), specials=[\"<unk>\", \"<pad>\"])\n",
        "trg_voc = build_vocab_from_iterator(yield_tokens(train_iter, 1), specials=[\"<unk>\", \"<pad>\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC27hmphuc_D"
      },
      "outputs": [],
      "source": [
        "src_voc.set_default_index(src_voc[\"<unk>\"])\n",
        "trg_voc.set_default_index(trg_voc[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RUgnWjVud9q",
        "outputId": "e6feac0d-3a8d-4e87-b8f9-a41f2e6740c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11379\n",
            "12103\n"
          ]
        }
      ],
      "source": [
        "print(len(src_voc))\n",
        "print(len(trg_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKzLmF3ou9Oj"
      },
      "outputs": [],
      "source": [
        "src_pipeline = lambda x: src_voc(x.split())\n",
        "trg_pipeline = lambda x: trg_voc(x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqBslvb1umwN"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAWPDkASvCjx"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgWUvr2qvEYa"
      },
      "outputs": [],
      "source": [
        "#collate_function: process the list of samples to form a batch. \n",
        "# https://androidkt.com/create-dataloader-with-collate_fn-for-variable-length-input-in-pytorch/\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    src_list, trg_list, src_len = [], [], []\n",
        "    for (_src, _trg) in batch:\n",
        "         processed_src = torch.tensor(src_pipeline(_src), dtype=torch.int64)\n",
        "         src_list.append(processed_src)\n",
        "         src_len.append(len(processed_src))\n",
        "         processed_trg = torch.tensor(trg_pipeline(_trg), dtype=torch.int64)\n",
        "         trg_list.append(processed_trg)\n",
        "    src_list = pad_sequence(src_list, padding_value = 1)\n",
        "    trg_list = pad_sequence(trg_list, padding_value = 1)\n",
        "    return src_list.to(device), trg_list.to(device), src_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D79M9QEvFne"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofgy0wL-BgLf"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyTL58x2vHpk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True) #GRU를 사용해서 cell state 받지 않음\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "                \n",
        "        #need to explicitly put lengths on cpu!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths = src_len, enforce_sorted = False)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "                                 \n",
        "        #packed_outputs is a packed sequence containing all hidden states\n",
        "        #hidden is now from the final non-padded element in the batch\n",
        "            \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\n",
        "        #  when the input is a pad token are all zeros\n",
        "            \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYX8EwQWvJcO"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "  \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim] #nn.Linear(ddec_hid_dim, 1)\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        \n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfVZ7SnmvKun"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask) #encoder의 output과 hidden state\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        ## 차원을 맞춰주기 위해 하는 과정 ##\n",
        "        weighted = torch.bmm(a, encoder_outputs) #batch matrix multiplication\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "        ##\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NfY6-OxvMAC"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5KFm1SavNM9"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(src_voc)\n",
        "OUTPUT_DIM = len(trg_voc)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX =src_voc(['<pad>'])[0]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxJfvKpwvOQy",
        "outputId": "69d08041-c9a2-45bd-8ceb-8a23e23e648a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(11379, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(12103, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=12103, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR68pvg6vPra",
        "outputId": "2e502570-ff8a-474d-9586-aaf7e21a7403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 34,145,351 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSBKgvE6vQrg"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oldSarlj0Utd"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX =  trg_voc(['<pad>'])[0]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuqJYh9rvU_8"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch[0], batch[2]\n",
        "        trg = batch[1]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUZ-5opNvWK2"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch[0], batch[2]\n",
        "            trg = batch[1]\n",
        "\n",
        "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzSw_OYPvXaC"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODBjUaVKBpfu"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QHvgv1vlyk",
        "outputId": "f2b47a36-9061-447b-f855-f6cdad93159b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 4m 15s\n",
            "\tTrain Loss: 3.826 | Train PPL:  45.880\n",
            "\t Val. Loss: 4.638 |  Val. PPL: 103.313\n",
            "Epoch: 02 | Time: 4m 19s\n",
            "\tTrain Loss: 2.215 | Train PPL:   9.160\n",
            "\t Val. Loss: 4.478 |  Val. PPL:  88.080\n",
            "Epoch: 03 | Time: 4m 18s\n",
            "\tTrain Loss: 1.699 | Train PPL:   5.466\n",
            "\t Val. Loss: 4.462 |  Val. PPL:  86.642\n",
            "Epoch: 04 | Time: 4m 16s\n",
            "\tTrain Loss: 1.418 | Train PPL:   4.128\n",
            "\t Val. Loss: 4.509 |  Val. PPL:  90.836\n",
            "Epoch: 05 | Time: 4m 18s\n",
            "\tTrain Loss: 1.225 | Train PPL:   3.405\n",
            "\t Val. Loss: 4.595 |  Val. PPL:  99.006\n",
            "Epoch: 06 | Time: 4m 17s\n",
            "\tTrain Loss: 1.089 | Train PPL:   2.970\n",
            "\t Val. Loss: 4.713 |  Val. PPL: 111.343\n",
            "Epoch: 07 | Time: 4m 17s\n",
            "\tTrain Loss: 0.998 | Train PPL:   2.713\n",
            "\t Val. Loss: 4.836 |  Val. PPL: 125.933\n",
            "Epoch: 08 | Time: 4m 19s\n",
            "\tTrain Loss: 0.912 | Train PPL:   2.488\n",
            "\t Val. Loss: 4.901 |  Val. PPL: 134.439\n",
            "Epoch: 09 | Time: 4m 19s\n",
            "\tTrain Loss: 0.842 | Train PPL:   2.320\n",
            "\t Val. Loss: 4.946 |  Val. PPL: 140.550\n",
            "Epoch: 10 | Time: 4m 18s\n",
            "\tTrain Loss: 0.775 | Train PPL:   2.171\n",
            "\t Val. Loss: 5.050 |  Val. PPL: 156.069\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGOGJ1aDm778"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psj0Jk-BytzM",
        "outputId": "902ebf33-091b-4908-86f2-cd3ae23eae5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 4.530 | Test PPL:  92.745 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut4-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK_8zIooBuFs"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUfuNySCDhFJ"
      },
      "outputs": [],
      "source": [
        "trg_itos = trg_voc.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h50_Qo1FS5w8"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_voc, trg_voc, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token for token in sentence.split()]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "        \n",
        "    src_indexes = [src_voc[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_voc['<sos>']]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                #.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_voc['<eos>']:\n",
        "            break\n",
        "        \n",
        "        #trg_indexes.append(pred_token)\n",
        "    \n",
        "    trg_tokens = [trg_itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGy5WKFHXFIG"
      },
      "outputs": [],
      "source": [
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVlT_2Y1WvUA",
        "outputId": "b9a520ad-3ae7-42c7-f1b8-3ec8be78f24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 . ['You', 'are', 'placed', 'placed', 'placed', 'placed', 'placed', 'placed', 'placed', ',', 'and', 'placed', ',', 'and', ',', 'and', ',', 'and', ',', 'and', ',', 'and', ',', 'and', ',', 'and', ',', 'and', ',', '.', '<eos>']\n",
            "미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ? ['Excuse', 'me', '.', 'I', \"'d\", 'like', 'to', 'check', 'with', 'a', '.', '.', 'I', \"'d\", 'like', 'to', 'change', 'my', 'bill', '.', '<eos>']\n",
            "은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요 ['The', 'price', 'is', 'the', '.', '.', 'It', 'will', 'be', 'good', '.', '.', '<eos>']\n",
            "아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ? ['I', \"'d\", 'like', 'to', 'fill', 'out', '.', 'I', \"'d\", 'like', 'to', '.', '.', 'Would', 'you', 'like', 'to', 'the', '?', '<eos>']\n",
            "부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 . ['I', \"'ve\", 'a', 'a', 'the', 'previous', 'previous', 'previous', 'due', 'to', 'the', '.', '.', '<eos>']\n",
            "변기 가 막히 었 습니다 . ['The', 'toilet', 'is', 'clogged', '.', '.', '<eos>']\n",
            "그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ? ['Could', 'you', 'show', 'me', 'the', 'pearl', '?', 'How', 'much', 'is', 'it', '?', '<eos>']\n",
            "비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 . ['I', \"'d\", 'like', 'to', 'buy', 'a', 'department', 'store', '.', 'I', \"'d\", 'like', 'to', 'see', 'the', '.', '.', '<eos>']\n",
            "속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다 ['I', \"'m\", 'not', 'eat', 'a', 'I', 'I', 'I', 'have', 'a', '.', '.', '<eos>']\n",
            "문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 . ['You', 'have', 'a', 'to', 'the', 'in', 'the', '.', '.', '<eos>']\n",
            "이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 . ['Please', 'you', 'connect', 'me', 'what', 'time', '.', '<eos>']\n",
            "이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 . ['We', 'are', 'having', 'a', 'for', 'a', '.', '.', '<eos>']\n",
            "통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 . ['I', \"'m\", 'a', 'a', 'of', 'the', ',', 'and', 'a', ',', 'and', ',', 'and', 'a', ',', 'and', 'a', '.', '<eos>']\n",
            "이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 . ['You', 'will', 'be', 'a', 'for', 'a', '.', '.', 'this', 'one', '.', '<eos>']\n",
            "요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다  ['This', 'is', 'a', 'fermented', 'for', 'a', '.', '.', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "for sent in sen_list:\n",
        "  translation, attention = translate_sentence(sent, src_voc, trg_voc, model, device)\n",
        "  print(sent, translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xcZF-W3Bxyq"
      },
      "source": [
        "## Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlN46mLwVGMp"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data, src_voc, trg_voc, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = datum[0]\n",
        "        trg = datum[1].split()\n",
        "\n",
        "        trg.pop(0)\n",
        "        trg.pop()\n",
        "        \n",
        "        pred_trg = translate_sentence(src, src_voc, trg_voc, model, device, max_len)\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append(trg)\n",
        "        \n",
        "    return corpus_bleu(pred_trgs, trgs, weights=(1, 0, 0, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNYo4IlXVUW3",
        "outputId": "26b5957d-925d-4b69-aa90-bcdddbfea592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score = 37.47\n"
          ]
        }
      ],
      "source": [
        "bleu_score = calculate_bleu(test_dataset, src_voc, trg_voc, model, device, max_len=50)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X2aAT8Hc2MYg"
      },
      "source": [
        "# Convolutional Seq to Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H-QtSon2VZ9"
      },
      "source": [
        "## 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuOeRDyt2VZ9",
        "outputId": "f6253eb4-96fc-4939-c6ca-62abd3ec3105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPv8J9nL2VZ-"
      },
      "source": [
        "## 파일 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNsNhRxc2VZ-"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Uc-6v62VZ-"
      },
      "outputs": [],
      "source": [
        "ko_path = PATH+\"ko-en.ko.parse\"\n",
        "en_path = PATH+\"ko-en.en.parse.syn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGtspuJh2VZ-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyZ608Pg2VZ_"
      },
      "outputs": [],
      "source": [
        "ko_lines = \"\"\n",
        "with open(ko_path, \"r\") as ko_file:\n",
        "    for line in ko_file.readlines():\n",
        "        ko_lines += line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qePiApWu2VZ_"
      },
      "outputs": [],
      "source": [
        "with open(en_path, \"r\") as en_file:\n",
        "    en_lines = en_file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lskr0XJI2VZ_"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHD16YLz2VZ_"
      },
      "outputs": [],
      "source": [
        "from nltk import Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477eQBmEKRGT"
      },
      "outputs": [],
      "source": [
        "# nltk의 Tree 모듈을 사용하여 필요한 정보 추출, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "full_en_text_list = []\n",
        "for line in en_lines:\n",
        "    sent = '<sos>' + ' '\n",
        "    t = Tree.fromstring(line)\n",
        "    for token in t.leaves():\n",
        "      sent += token + ' '\n",
        "    sent += '<eos>'\n",
        "    full_en_text_list.append(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj_ru2RK2VaA",
        "outputId": "44d2cb51-67c6-407f-d159-8b5250943b25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> Flight 007 will stay on the ground for one hour . <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>',\n",
              " \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\",\n",
              " '<sos> The official exchange rate is around 1,250 Won . <eos>',\n",
              " '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>',\n",
              " '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>',\n",
              " '<sos> Do you have change for $ 100 ? <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " \"<sos> I 'd like to change $ 100 . <eos>\",\n",
              " '<sos> Change 100 dollars . <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " '<sos> One hundred dollars . <eos>',\n",
              " \"<sos> I want four 100 's , two 20 's , five 10 's and ten 1 's . <eos>\",\n",
              " '<sos> 6 ten dollar bills and 8 five dollar bills , please . <eos>',\n",
              " '<sos> Could I have change for a one-hundred dollar bill ? <eos>',\n",
              " \"<sos> I 'd like to change one hundred . <eos>\",\n",
              " '<sos> I want four hundreds , three twenties , three tens , one five , and five ones . <eos>',\n",
              " '<sos> 100 miles is credited to your account . <eos>',\n",
              " '<sos> Five tens , and ten twenties , please . <eos>',\n",
              " '<sos> About 10 dollars . <eos>',\n",
              " '<sos> I want seven ten dollar bills and thirty one dollar bills . <eos>',\n",
              " '<sos> Two ten dollar bills , and two five dollar bills please . <eos>',\n",
              " '<sos> Make it 30 ten-dollar bills and small change . <eos>',\n",
              " '<sos> 8 ten-dollar bills and 4 five dollar bills , please . <eos>',\n",
              " '<sos> Fifty 10 dollar bills , please . <eos>',\n",
              " '<sos> Can I have change for this 10 dollar bill ? <eos>',\n",
              " '<sos> Could you give me small change for a ten dollar bill ? <eos>',\n",
              " '<sos> I will pay $ 10 in cash and the rest in credit card . <eos>',\n",
              " '<sos> 10dollars . <eos>',\n",
              " '<sos> Ten ten-dollar bills and ten one-dollar bills , please . <eos>',\n",
              " '<sos> Ten tens , ten fives , and fifty ones , please . <eos>',\n",
              " '<sos> 10 ten-dollar bills and 100 one-dollar bills , please . <eos>',\n",
              " '<sos> Twenty 10 dollar bills , forty 5 dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 's , two 5 's and ten 1 's . <eos>\",\n",
              " '<sos> Four tens , four fives and the rest in singles , please . <eos>',\n",
              " '<sos> 4 tens , and the rest in one dollar bills . <eos>',\n",
              " '<sos> 7 ten dollar bills and 6 five dollar bills , please . <eos>',\n",
              " '<sos> 2 tens and the rest in one dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 dollar bills , and the rest in 1 dollar bills . <eos>\",\n",
              " '<sos> Ten tens , five twenties and coins for the rest , please . <eos>',\n",
              " \"<sos> I 'd like 10 ten-dollar bills , 20 five-dollar bills and 30 one-dollar bills . <eos>\",\n",
              " '<sos> 10dollar bills , please . <eos>',\n",
              " '<sos> Could you give me ten one-dollar bill for this ten dollar bill ? <eos>',\n",
              " '<sos> Thirty ten-dollar bills and small change for ten dollars . <eos>',\n",
              " '<sos> Please change this for ten dollar bills . <eos>',\n",
              " '<sos> Ten dollar bills , please . <eos>',\n",
              " '<sos> Would you like tens or twenties ? <eos>',\n",
              " '<sos> In tens and twenties . <eos>',\n",
              " '<sos> 5 ten dollar bills , 15 five-dollar bills and 20 singles . <eos>',\n",
              " '<sos> I need six tens and small change for the rest . <eos>',\n",
              " \"<sos> I 'd like to have 9 tens and the rest in fives . <eos>\",\n",
              " '<sos> Could you change this 10 dollar bill for me ? <eos>',\n",
              " \"<sos> I 'd like to have some tens and twenties . <eos>\",\n",
              " '<sos> In 10 and 20 dollar bills . <eos>',\n",
              " '<sos> Do you like it in dimes ? <eos>',\n",
              " '<sos> This is 10 kilograms overweight . <eos>',\n",
              " '<sos> This is 12 kilograms overweight . <eos>',\n",
              " '<sos> There are 10 kinds of them . <eos>',\n",
              " '<sos> To upgrade to the diamond level requires 100,000 points or more . <eos>',\n",
              " \"<sos> It 's Gate 10 . <eos>\",\n",
              " '<sos> Could you tell me the way to Gate 10 ? <eos>',\n",
              " '<sos> Please proceed to Gate 10 . <eos>',\n",
              " '<sos> It will depart in ten minutes . <eos>',\n",
              " '<sos> 10 minutes . Never mind . <eos>',\n",
              " '<sos> October 11th . <eos>',\n",
              " '<sos> We reserved a seat for October 25th , eleven a.m. <eos>',\n",
              " \"<sos> I 'd like to change the date to October 9th . <eos>\",\n",
              " \"<sos> I 'll be here for ten days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> For about ten days . <eos>',\n",
              " \"<sos> I 'll stay for 10 days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> 10 days . <eos>',\n",
              " '<sos> For ten days . <eos>',\n",
              " '<sos> We have 10 different brands . <eos>',\n",
              " '<sos> Where is Gate 11 ? <eos>',\n",
              " '<sos> Would you tell me how to get to Gate 11 ? <eos>',\n",
              " '<sos> It departs at 11:00 . <eos>',\n",
              " '<sos> How about the 11th ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> 12G is over there on the right side . <eos>',\n",
              " '<sos> Please go to Gate 12 . <eos>',\n",
              " '<sos> Please come to Gate 12 . <eos>',\n",
              " \"<sos> It 's time for boarding now at Gate 12 . <eos>\",\n",
              " '<sos> Boarding begins at 12:10 . <eos>',\n",
              " '<sos> The arrival time is twelve thirty local time . <eos>',\n",
              " \"<sos> Let me have two seats on the 12 o'clock flight . <eos>\",\n",
              " '<sos> I want to cancel my December 25 flight . <eos>',\n",
              " '<sos> I want to make it on December 27 . <eos>',\n",
              " '<sos> Please cancel my flight for December 4th . <eos>',\n",
              " \"<sos> I 'd like to change the date to December 7th . <eos>\",\n",
              " '<sos> Go to gate 13 , please . <eos>',\n",
              " \"<sos> There 's 13 hour 's difference . <eos>\",\n",
              " '<sos> One hundred forty dollars . Which seat do you prefer ? <eos>',\n",
              " \"<sos> There 's a 14 hour 's difference between two cities . Seoul is 14 hours ahead . <eos>\",\n",
              " '<sos> 15dollars , please . <eos>',\n",
              " '<sos> Where is seat 15A ? <eos>',\n",
              " '<sos> Your flight will depart from gate 15 . <eos>',\n",
              " \"<sos> There 's a 15 hour 's difference . Seoul is 14 hours ahead . <eos>\"]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_en_text_list[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0zXFSo2VaA",
        "outputId": "38c03bcc-8325-417b-a85f-fee9ee3fb41c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<id 1>',\n",
              " '<sent 1>',\n",
              " '1\\t2\\tNP\\t777/SN',\n",
              " '2\\t6\\tNP_SBJ\\t항공편/NNG|은/JX',\n",
              " '3\\t4\\tNP\\t1/SN|시간/NNG',\n",
              " '4\\t6\\tNP_AJT\\t동안/NNG',\n",
              " '5\\t6\\tNP_AJT\\t지상/NNG|에/JKB',\n",
              " '6\\t7\\tVP\\t머물/VV|게/EC',\n",
              " '7\\t0\\tVP\\t되/VV|ㅂ니다/EF|./SF',\n",
              " '</sent>',\n",
              " '']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 한 문장씩 나눈 리스트 만들기\n",
        "ko_list = ko_lines.split(\"</id>\")\n",
        "for i in range(len(ko_list)):\n",
        "    ko_list[i] = ko_list[i].split(\"\\n\")\n",
        "\n",
        "ko_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASAtTz6y2VaA"
      },
      "outputs": [],
      "source": [
        "# 필요한 정보만 추출\n",
        "import re\n",
        "\n",
        "pattern = r\"[가-힣ㄱ-ㅎ]+|[0-9]+(?=\\/SN)\"\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    for j in range(len(ko_list[i])):\n",
        "        ko_list[i][j] = re.findall(pattern, ko_list[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbUoKAIn2VaA",
        "outputId": "46448e13-be19-4a76-8f11-3a242cdd7de2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[[],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['1', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['머물', '게'],\n",
              "  ['되', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['3', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['있', '겠', '습니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['1', '000', '달러'],\n",
              "  ['여행자', '수표', '가'],\n",
              "  ['필요', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['1', '250', '원', '이'], ['공식'], ['환율', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['3', '장', '과'],\n",
              "  ['나머지', '는'],\n",
              "  ['20', '달러', '권', '으로'],\n",
              "  ['주', '시', 'ㅂ시오'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['한'],\n",
              "  ['장', '과'],\n",
              "  ['50', '달러'],\n",
              "  ['4', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '를'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '만'], ['바꾸', '어'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전'],\n",
              "  ['좀'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['어', '주', '어', '요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['6', '개'],\n",
              "  ['하', '고'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['8', '개', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '겠', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '불', '을'], ['바꾸', '겠', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '불', '짜리'],\n",
              "  ['4', '매'],\n",
              "  ['20', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['1', '매'],\n",
              "  ['그리고'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['5', '매'],\n",
              "  ['원하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100'], ['마일리지', '가'], ['적립', '되', '었', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['5', '장', '과'],\n",
              "  ['20', '달러'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['부탁', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러'], ['정도', '이', '에요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['7', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['서른'],\n",
              "  ['장', '과'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['여덟'],\n",
              "  ['장하', '고'],\n",
              "  ['5', '달러'],\n",
              "  ['지폐'],\n",
              "  ['네'],\n",
              "  ['장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '로'],\n",
              "  ['50', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ', '려고요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '는'],\n",
              "  ['현금', '으로'],\n",
              "  ['하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['카드', '로'],\n",
              "  ['계산', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['그리고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['50', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장', '하', '고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['100', '장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['20', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['40', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['3', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장', '하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['7', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['6', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['세'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '과'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['다섯'],\n",
              "  ['장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['동전', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['20', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리'], ['지폐', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '가'],\n",
              "  ['서른'],\n",
              "  ['10', '달러', '는'],\n",
              "  ['잔돈', '이', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  ['아니', '면'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '와'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['15', '장'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['20', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['6', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_list[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECNmeIGysBQ7"
      },
      "outputs": [],
      "source": [
        "# 문장으로 만들어서 리스트에 넣어주기, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "ko_text_list = []\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    full_sent = '<sos>' + ' '\n",
        "    for j in range(len(ko_list[i])):\n",
        "        if ko_list[i][j]:\n",
        "            for token in ko_list[i][j]:\n",
        "                full_sent += token + ' '\n",
        "    full_sent += '<eos>'\n",
        "\n",
        "    ko_text_list.append(full_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmXRPHoX2VaA",
        "outputId": "a89f85cd-3099-443b-e316-6418db05d113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>',\n",
              " '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>',\n",
              " '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 100 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 짜리 4 장 20 달러 짜리 2 장 10 달러 짜리 5 장 1 달러 짜리 10 장 으로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 6 개 하 고 5 달러 짜리 지폐 8 개 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 겠 습니까 <eos>',\n",
              " '<sos> 100 불 을 바꾸 겠 습니다 <eos>',\n",
              " '<sos> 100 불 짜리 4 매 20 불 짜리 3 매 10 불 짜리 3 매 5 불 짜리 1 매 그리고 1 불 짜리 5 매 원하 ㅂ니다 <eos>',\n",
              " '<sos> 100 마일리지 가 적립 되 었 습니다 <eos>',\n",
              " '<sos> 10 달러 5 장 과 20 달러 10 장 으로 부탁 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 정도 이 에요 <eos>',\n",
              " '<sos> 10 달러 지폐 7 장 1 달러 지폐 30 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 두 장 5 달러 짜리 두 장 주 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 서른 장 과 잔돈 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 여덟 장하 고 5 달러 지폐 네 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 로 50 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 습니까 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 려고요 <eos>',\n",
              " '<sos> 10 달러 는 현금 으로 하 고 나머지 는 카드 로 계산 하 아 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 5 달러 짜리 10 장 그리고 1 달러 짜리 50 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 하 고 1 달러 짜리 100 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 20 장 5 달러 짜리 40 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 3 장 5 달러 짜리 2 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 5 달러 짜리 4 장 그리고 나머지 는 1 달러 짜리 로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 하 고 나머지 는 1 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 7 장 5 달러 짜리 6 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 두 장이 랑 나머지 는 1 달러 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 세 장이 랑 나머지 는 1 달러 로 주 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 열 장 과 20 달러 짜리 다섯 장 그리고 나머지 는 동전 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 10 장 5 달러 짜리 지폐 20 장 1 달러 지폐 30 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 를 1 달러 짜리 열 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 가 서른 10 달러 는 잔돈 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 하 ㄹ까요 아니 면 20 달러 짜리 로 하 ㄹ까요 <eos>',\n",
              " '<sos> 10 달러 짜리 와 20 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 5 장 5 불 짜리 15 장 1 불 짜리 20 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 6 장 그리고 나머지 는 잔돈 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 9 장 나머지 는 5 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 10 불 짜리 와 20 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 하 고 20 불 짜리로요 <eos>',\n",
              " '<sos> 10 센트 로 드리 ㄹ 까요 <eos>',\n",
              " '<sos> 10 무게 초과 되 시 었 습니다 <eos>',\n",
              " '<sos> 12 초과 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 가지 종류 가 있 는데요 <eos>',\n",
              " '<sos> 10 만 점 이상 부터 다이아몬드 등급 으로 상향 조정 되 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 게이트 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 출구 로 가 는 길 을 알리 러 주 시 겠 습니까 <eos>',\n",
              " '<sos> 10 번 출구 로 가 시 어요 <eos>',\n",
              " '<sos> 10 분 후 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 분 이 ㅂ니다 신경 쓰 지 말 시 어요 <eos>',\n",
              " '<sos> 10 월 11 일 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 월 25 일 오전 11 시 발 비행 편 에 예약 되 었 습니다 <eos>',\n",
              " '<sos> 10 월 9 일 로 변경 하 고 싶 은데요 <eos>',\n",
              " '<sos> 10 일 동안 있 을 거 이 에요 <eos>',\n",
              " '<sos> 열흘 동안 이 요 <eos>',\n",
              " '<sos> 십 일 정도 <eos>',\n",
              " '<sos> 열흘 간 머무르 ㄹ 려구요 <eos>',\n",
              " '<sos> 십 일 간 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 종류 의 상표 가 있 어요 <eos>',\n",
              " '<sos> 11 번 게이트 는 어디 에 있 나요 <eos>',\n",
              " '<sos> 11 번 게이트 에 어떻 게 가 는지 알리 러 주 시 겠 어요 <eos>',\n",
              " '<sos> 11 시 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 11 일 은 어떻 어요 <eos>',\n",
              " '<sos> 12 자리 가 어디 죠 <eos>',\n",
              " '<sos> 12 좌석 은 어디 이 ㅂ니까 <eos>',\n",
              " '<sos> 12 좌석 은 저기 오른쪽 에 있 습니다 <eos>',\n",
              " '<sos> 12 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 12 번 게이트 로 오 시 어요 <eos>',\n",
              " '<sos> 12 번 창구 에서 지금 탑승 하 시 ㅂ시오 <eos>',\n",
              " '<sos> 12 시 10 분 에 탑승 을 시작 하 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 30 분 에 도착 예정 이 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 편 두 장 주 시 어요 <eos>',\n",
              " '<sos> 12 월 25 일 편 을 취소 하 고 싶 은데요 <eos>',\n",
              " '<sos> 12 월 27 일 로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 12 월 4 일 예약 취소 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 12 월 7 일 로 날짜 를 바꾸 고 싶 은데요 <eos>',\n",
              " '<sos> 13 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 13 시간 의 시차 가 있 습니다 <eos>',\n",
              " '<sos> 140 달러 이 ㅂ니다 어느 쪽 좌석 을 원하 시 어요 <eos>',\n",
              " '<sos> 14 시간 의 시차 가 있 습니다 서울 이 14 시간 빠르 죠 <eos>',\n",
              " '<sos> 15 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 15 좌석 은 어디 이 에요 <eos>',\n",
              " '<sos> 15 번 탑승구 는 번 중앙 홀 에 있 습니다 <eos>',\n",
              " '<sos> 15 시간 이 ㅂ니다 서울 이 14 시간 빠릅 니다 <eos>']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_text_list[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ9XEFRE2VaB"
      },
      "source": [
        "## 한-영 병행 데이터 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqRVz98s2VaB",
        "outputId": "619ae896-204d-4936-80ee-eac22976398d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<sos> Flight 007 will stay on the ground for one hour . <eos>', '<sos> Flight 017 will stay on the ground for three hours . <eos>', \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\", '<sos> The official exchange rate is around 1,250 Won . <eos>', '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>', '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>', '<sos> Do you have change for $ 100 ? <eos>', \"<sos> I 'd like to change 100 dollars . <eos>\", \"<sos> I 'd like to change $ 100 . <eos>\", '<sos> Change 100 dollars . <eos>']\n",
            "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>', '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>', '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>', '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>', '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>', '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>', '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>', '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>']\n"
          ]
        }
      ],
      "source": [
        "print(full_en_text_list[:10])\n",
        "print(ko_text_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USTV5ikY2VaB"
      },
      "outputs": [],
      "source": [
        "parallel_data = list(zip(ko_text_list, full_en_text_list)) # src:한국어, trg:영어 순으로 넣어줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STomJ-4T2VaB",
        "outputId": "96d2d93b-9852-4254-9b39-faca552e90ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> Flight 007 will stay on the ground for one hour . <eos>')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjfEAl3X2VaB",
        "outputId": "aabaf700-c183-4a1f-da3c-8116722e3aa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwH1NSlPxhAA",
        "outputId": "f723671b-5810-4064-c0be-9c68215cd9a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "330974"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parallel_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikopDavGgcmt"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMnAjY2Vz9X3",
        "outputId": "e0ca9cf0-8394-4fa4-a284-6b7077ab55ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "import portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xbLSm6OfVwQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bgkzIdNJfWad",
        "outputId": "5f2e3d38-6275-48d5-bd6e-6b156bc24bfc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.15.2+cpu'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLMR7Ryjf1_C"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "# 8:1:1로 train, valid, test 나눔\n",
        "TRAIN_RATE = 0.8\n",
        "VALID_RATE = 0.9\n",
        "\n",
        "train_iter, valid_iter, test_iter = parallel_data[:int(len(parallel_data)*TRAIN_RATE)], parallel_data[int(len(parallel_data)*TRAIN_RATE):int(len(parallel_data)*VALID_RATE)], parallel_data[int(len(parallel_data)*VALID_RATE):]\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "valid_dataset = to_map_style_dataset(valid_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7KwlFUyshnA",
        "outputId": "7d5c6a16-1017-4c56-c457-82fdebb3a1b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> Flight 007 will stay on the ground for one hour . <eos>')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa9_iS7R6hxQ",
        "outputId": "4ff3b74f-e720-486e-d58b-78e42ab83e6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 한식 을 좋아하 ㅂ니까 <eos>', '<sos> Do you like Korean food ? <eos>')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd_S8XEDgwJ4"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(file_iter, type = 0):\n",
        "     if type == 0:\n",
        "        for line in file_iter:\n",
        "            yield line[0].split()\n",
        "     else:\n",
        "        for line in file_iter:\n",
        "            yield line[1].split()\n",
        "\n",
        "src_voc = build_vocab_from_iterator(yield_tokens(train_iter, 0), specials=[\"<unk>\", \"<pad>\"])\n",
        "trg_voc = build_vocab_from_iterator(yield_tokens(train_iter, 1), specials=[\"<unk>\", \"<pad>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pGBy0TvhCEN",
        "outputId": "3bf9accd-cdc4-4e36-b4e0-b8bf1b7cef9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29684\n",
            "33549\n"
          ]
        }
      ],
      "source": [
        "print(len(src_voc))\n",
        "print(len(trg_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAjqfQNnhJ1l"
      },
      "outputs": [],
      "source": [
        "src_voc.set_default_index(src_voc[\"<unk>\"])\n",
        "trg_voc.set_default_index(trg_voc[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOCA3zEjhMVB"
      },
      "outputs": [],
      "source": [
        "src_pipeline = lambda x: src_voc(x.split())\n",
        "trg_pipeline = lambda x: trg_voc(x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtZvyMqWhPtw",
        "outputId": "0f952e08-248b-4b10-cc7a-5c46dcad35e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUDX_EE2hRMR"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    src_list, trg_list, src_len = [], [], []\n",
        "    for (_src, _trg) in batch:\n",
        "        processed_src = torch.tensor(src_pipeline(_src), dtype=torch.int64)\n",
        "        src_list.append(processed_src)\n",
        "        src_len.append(len(processed_src))\n",
        "        processed_trg = torch.tensor(trg_pipeline(_trg), dtype=torch.int64)\n",
        "        trg_list.append(processed_trg)\n",
        "    src_list = pad_sequence(src_list, batch_first=True, padding_value=1)\n",
        "    trg_list = pad_sequence(trg_list, batch_first=True, padding_value=1)\n",
        "    # batch_first=True로 설정해서 이후 forward에서 permute 안 해도 됨\n",
        "    return src_list.to(device), trg_list.to(device), src_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztw92hoXhTUT"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCMXW6IUhYC-"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwiRxLEahWx0"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim) # token에 대한 정보\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim) # positional 한 정보\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim) # emb_dim을 hid_dim으로 만들어줌\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim) # hid_dim을 emb_dim으로 만들어줌\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size, \n",
        "                                              padding = (kernel_size - 1) // 2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "\n",
        "        # src = src.permute(1,0) # 코드 삭제\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
        "\n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, src len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
        "\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "\n",
        "        #conv_input = [batch size, src len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) # convolutional netword에서 요구하는 tensor size로 맞춰주기 위함\n",
        "        \n",
        "        #conv_input = [batch size, hid dim, src len]\n",
        "        \n",
        "        #begin convolutional blocks...\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, src len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration # 결과로 나온 것을 input으로 다시 들어갈 수 있도록 함\n",
        "            conv_input = conved\n",
        "        \n",
        "        #...end convolutional blocks\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        return conved, combined # conved: residual connection 적용X / combined: residual connection 적용O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtINW8IHha-k"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        #conved = [batch size, hid dim, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved_emb = [batch size, trg len, emb dim]\n",
        "        \n",
        "        combined = (conved_emb + embedded) * self.scale\n",
        "\n",
        "        # 디버깅용 코드\n",
        "        # print(\"0. conved_emb: \", end='')\n",
        "        # print(conved_emb.size())\n",
        "\n",
        "        # print(\"0. embedded: \", end='')\n",
        "        # print(embedded.size())\n",
        "\n",
        "        #combined = [batch size, trg len, emb dim]\n",
        "\n",
        "        # print(\"1. combined: \", end='')\n",
        "        # print(combined.size())\n",
        "\n",
        "        # print(\"2. encoder_conved_permute: \", end='')\n",
        "        # print(encoder_conved.permute(0,2,1).size())\n",
        "                \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "\n",
        "        # print(\"3. energy: \", end='')\n",
        "        # print(energy.size())\n",
        "        \n",
        "        #energy = [batch size, trg len, src len]\n",
        "        \n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        #attention = [batch size, trg len, src len]\n",
        "            \n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, emd dim]\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #apply residual connection\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "\n",
        "        # trg = trg.permute(1,0) # 코드 삭제\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "            \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, trg len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = [batch size, trg len, emb dim]\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, trg len]\n",
        "        \n",
        "        batch_size = conv_input.shape[0]\n",
        "        hid_dim = conv_input.shape[1]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\"\n",
        "            padding = torch.zeros(batch_size, \n",
        "                                  hid_dim, \n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
        "                \n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
        "        \n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\n",
        "            \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #calculate attention\n",
        "            attention, conved = self.calculate_attention(embedded, \n",
        "                                                         conved, \n",
        "                                                         encoder_conved, \n",
        "                                                         encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg len, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "         \n",
        "        #conved = [batch size, trg len, emb dim]\n",
        "            \n",
        "        output = self.fc_out(self.dropout(conved))\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml02Tv_ahcze"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
        "           \n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
        "        #  positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "            \n",
        "        #encoder_conved = [batch size, src len, emb dim]\n",
        "        #encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for \n",
        "        #  each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #attention = [batch size, trg len - 1, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U_hZB3OhfMN"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(src_voc)\n",
        "OUTPUT_DIM = len(trg_voc)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "TRG_PAD_IDX = trg_voc(['<pad>'])[0]\n",
        "\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
        "\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdiNzTE8hgPL",
        "outputId": "0dec435c-8395-4377-dd28-e4c93332d115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 57,127,437 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdvcakWKhhxQ"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brq8VuDKhjP1"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pamI_G7hkWA"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch[0]\n",
        "        trg = batch[1]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhUfAD_9hlnG"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-fUxgfchmzo"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iplhbapZ4Tum"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBpryJJ9hoda",
        "outputId": "1e5d3f44-8399-415a-942d-2cbac57ba221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 12m 17s\n",
            "\tTrain Loss: 3.186 | Train PPL:  24.180\n",
            "\t Val. Loss: 2.744 |  Val. PPL:  15.543\n",
            "Epoch: 02 | Time: 12m 6s\n",
            "\tTrain Loss: 2.266 | Train PPL:   9.646\n",
            "\t Val. Loss: 2.353 |  Val. PPL:  10.521\n",
            "Epoch: 03 | Time: 12m 5s\n",
            "\tTrain Loss: 2.009 | Train PPL:   7.458\n",
            "\t Val. Loss: 2.144 |  Val. PPL:   8.533\n",
            "Epoch: 04 | Time: 12m 7s\n",
            "\tTrain Loss: 1.851 | Train PPL:   6.365\n",
            "\t Val. Loss: 1.987 |  Val. PPL:   7.297\n",
            "Epoch: 05 | Time: 12m 8s\n",
            "\tTrain Loss: 1.732 | Train PPL:   5.653\n",
            "\t Val. Loss: 1.876 |  Val. PPL:   6.529\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5 \n",
        "# 너무 오래 걸리고, 과적합이 발생하여 epoch를 10까지 돌리면 중간에 loss가 늘어나는 문제 발생\n",
        "# 또한, epoch 5 즈음부터는 loss가 줄어드는 비율이 월등히 낮아지는 등의 학습에 큰 영향을 주는 일이 없는 것 같아 epoch 5까지만 돌림\n",
        "CLIP = 0.1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhH67pnNeO12"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ycbFl_eSEE",
        "outputId": "ab79f3dd-592d-48b8-c972-76a281ad33dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.791 | Test PPL:   5.995 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rACW-SZuBjsf"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbz8ItARJ4SV"
      },
      "outputs": [],
      "source": [
        "trg_itos = trg_voc.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjJtyUVYrohq"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_voc, trg_voc, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token for token in sentence.split()]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "        \n",
        "    src_indexes = [src_voc[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [trg_voc['<sos>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
        "                \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "\n",
        "        if pred_token == trg_voc['<eos>']:\n",
        "            break\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "    \n",
        "    trg_tokens = [trg_itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_w_nF6nBjsg"
      },
      "outputs": [],
      "source": [
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqSlg7DelEDs",
        "outputId": "818ab9dc-4749-4a54-9006-f6087a344588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted trg = ['I', 'have', 'to', 'put', 'them', 'of', 'liquids', ',', 'and', 'aerosols', ',', 'and', 'aerosols', ',', 'and', 'aerosols', ',', 'and', 'a', 'plastic', 'bag', '.']\n",
            "predicted trg = ['I', \"'m\", 'sorry', ',', 'but', 'I', \"'d\", 'like', 'to', 'have', 'a', 'little', 'meeting', 'of', 'the', 'speed', '.', 'May', 'I', 'have', 'a', 'ticket', 'for', 'a', 'few', 'distance', '?']\n",
            "predicted trg = ['The', 'bank', 'is', 'too', 'far', 'from', 'the', 'bank', '.', 'I', 'need', 'a', 'little', '.', 'I', 'need', 'to', 'be', 'a', 'little', '.']\n",
            "predicted trg = ['I', 'think', 'I', 'have', 'lost', 'lost', 'the', 'lost', ',', 'but', 'I', 'have', 'to', 'have', 'a', 'lost', 'of', 'the', 'lost', '.', 'Do', 'you', 'want', 'to', 'have', 'a', 'lost', 'of', 'the', 'lost', '?', 'I', 'need', 'to', 'be', 'out', 'with', 'the', 'lost', '.', 'Do', 'you', 'have', 'to', 'put', 'the', 'lost', 'lost', 'or', 'I']\n",
            "predicted trg = ['I', \"'m\", 'going', 'to', 'get', 'a', 'few', 'minute', 'call', 'from', 'Busan', ',', 'I', 'can', 'Daegu', 'a', 'few', 'minute', '.']\n",
            "predicted trg = ['The', 'toilet', 'was', 'blocked', '.', 'It', 'was', 'caught', '.']\n",
            "predicted trg = ['Please', 'show', 'me', 'that', 'pants', ',', 'please', '.', 'How', 'much', 'can', 'I', 'buy', 'it', '?']\n",
            "predicted trg = ['I', \"'d\", 'like', 'to', 'go', 'to', 'Macy', 'on', 'the', 'department', 'department', '.', 'I', \"'d\", 'like', 'to', 'go', 'to', 'Duta', ',', 'I', \"'d\", 'like', 'to', 'go', 'to', 'the', 'Duta', '.']\n",
            "predicted trg = ['I', 'do', \"n't\", 'feel', 'like', 'to', 'have', 'a', 'good', 'breakfast', ',', 'but', 'I', 'do', \"n't\", 'have', 'to', 'be', 'able', 'to', 'have', 'a', 'good', 'breakfast', '.']\n",
            "predicted trg = ['The', 'door', 'President', 'was', 'ready', 'to', 'get', 'a', 'little', 'profit', '.', 'I', 'told', 'you', '.']\n",
            "predicted trg = ['I', \"'d\", 'like', 'to', 'have', 'some', 'time', 'to', 'see', 'this', '.']\n",
            "predicted trg = ['The', 'average', 'was', 'all', 'a', 'little', 'bit', 'of', 'Oeinmyoji', 'of', 'Oeinmyoji', 'of', 'Oeinmyoji', '.']\n",
            "predicted trg = ['I', \"'ve\", 'been', 'sorry', 'of', 'the', 'average', 'of', 'the', 'delay', 'of', 'the', 'delay', 'of', 'the', 'delay', '.']\n",
            "predicted trg = ['I', 'think', 'this', 'product', 'is', 'made', 'to', 'be', 'a', 'little', 'of', 'the', 'company', 'of', 'the', 'sales', 'of', 'the', 'company', ',', 'and', 'I', \"'ll\", 'understand', 'you', '.']\n",
            "predicted trg = ['Nowadays', 'is', 'a', 'program', 'of', 'the', 'program', 'of', 'the', 'program', 'of', 'the', 'program', '.']\n"
          ]
        }
      ],
      "source": [
        "for sent in sen_list:\n",
        "    translation, attention = translate_sentence(sent, src_voc, trg_voc, model, device)\n",
        "    print(f'predicted trg = {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efzD1-RFBjsf"
      },
      "source": [
        "## Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHg9N_xDBjsh"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data, src_voc, trg_voc, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = datum[0]\n",
        "        trg = datum[1].split()\n",
        "\n",
        "        # <sos>, <eos> 제거\n",
        "        trg.pop(0)\n",
        "        trg.pop()\n",
        "        \n",
        "        pred_trg = translate_sentence(src, src_voc, trg_voc, model, device)\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append(trg)\n",
        "        \n",
        "    return corpus_bleu(pred_trgs, trgs, weights=(1, 0, 0, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcqFqOUOmAZD",
        "outputId": "bd68cc45-f5d0-4dbb-dcae-b6c8c251549a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score = 48.12\n"
          ]
        }
      ],
      "source": [
        "bleu_score = calculate_bleu(test_dataset, src_voc, trg_voc, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "duDExnkD2VB2"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2EV2fPNqo_y"
      },
      "source": [
        "transformers 모델만 특이하게 bleu score가 낮은 문제를 겪고 있었는데, dropout, normalization 등을 실행해보고 해결되지 않아 조교님께 질문드렸더니 다음과 같은 대처법을 알려주셨습니다.\n",
        "\n",
        "* 데이터셋이 배치 단위로 올바르게 훈련이 되는 지 뽑아보기\n",
        "\n",
        ": dataloader를 생성하고 대표로 train_dataset에서 첫번째 batch를 뽑아 형태를 확인하였으며, 문제가 없었습니다.\n",
        "\n",
        "* 훈련 과정에서 validation set에 대한 loss 수렴이 잘 이뤄지고 있는지 판단하기\n",
        "\n",
        ": validation set에 대한 loss 수렴은 잘 이뤄졌습니다.\n",
        "\n",
        "* 모든 훈련 스텝에 문제가 없다면 bleu score를 측정하는 내부 코드를 살펴보기\n",
        "\n",
        ": bleu score를 측정하는 내부 코드를 확인하였으며, 이는 학우들과 상의하여 수정한 코드로, 똑같은 코드에 대해서 학우들은 정상적으로 bleu score가 산출되었습니다.\n",
        "\n",
        "\n",
        "\n",
        "말씀해주신 대처법을 포함해 다양한 방안에서 문제를 해결해보고자 노력하였는데, 코드에서 문제를 발견하지 못했고, transformer의 경우에만 특이하게 bleu score가 낮은 원인을 파악하지 못하였습니다.\n",
        "\n",
        "어떤 부분이 문제가 되어 bleu score가 낮게 나온 것인지 알려주신다면 학습에 큰 도움이 될 것이라 생각합니다. 감사합니다. kyky8392@snu.ac.kr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OXCV4EjLqer"
      },
      "source": [
        "## 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkNueAHcLtem",
        "outputId": "c0c8692c-f88a-4b7f-8187-87ada9788f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOD5-mOLt2B"
      },
      "source": [
        "## 파일 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaD961clLu2J"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjzcQe-4LwI4"
      },
      "outputs": [],
      "source": [
        "ko_path = PATH+\"ko-en.ko.parse\"\n",
        "en_path = PATH+\"ko-en.en.parse.syn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibuPpnyJLy3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrNaAeovL2sC"
      },
      "outputs": [],
      "source": [
        "ko_lines = \"\"\n",
        "with open(ko_path, \"r\") as ko_file:\n",
        "    for line in ko_file.readlines():\n",
        "        ko_lines += line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkqmnl2XL30x"
      },
      "outputs": [],
      "source": [
        "with open(en_path, \"r\") as en_file:\n",
        "    en_lines = en_file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXW0-Ch6L4s0"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTtzjgeCL5x-"
      },
      "outputs": [],
      "source": [
        "from nltk import Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho4_Fmu6L7bp"
      },
      "outputs": [],
      "source": [
        "# nltk의 Tree 모듈을 사용하여 필요한 정보 추출, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "full_en_text_list = []\n",
        "for line in en_lines:\n",
        "    sent = '<sos>' + ' '\n",
        "    t = Tree.fromstring(line)\n",
        "    for token in t.leaves():\n",
        "      sent += token + ' '\n",
        "    sent += '<eos>'\n",
        "    full_en_text_list.append(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEl5YwNiL8bH",
        "outputId": "107fea8f-2c81-48ae-f26d-21c5c4b21a35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> Flight 007 will stay on the ground for one hour . <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>',\n",
              " \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\",\n",
              " '<sos> The official exchange rate is around 1,250 Won . <eos>',\n",
              " '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>',\n",
              " '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>',\n",
              " '<sos> Do you have change for $ 100 ? <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " \"<sos> I 'd like to change $ 100 . <eos>\",\n",
              " '<sos> Change 100 dollars . <eos>',\n",
              " \"<sos> I 'd like to change 100 dollars . <eos>\",\n",
              " '<sos> One hundred dollars . <eos>',\n",
              " \"<sos> I want four 100 's , two 20 's , five 10 's and ten 1 's . <eos>\",\n",
              " '<sos> 6 ten dollar bills and 8 five dollar bills , please . <eos>',\n",
              " '<sos> Could I have change for a one-hundred dollar bill ? <eos>',\n",
              " \"<sos> I 'd like to change one hundred . <eos>\",\n",
              " '<sos> I want four hundreds , three twenties , three tens , one five , and five ones . <eos>',\n",
              " '<sos> 100 miles is credited to your account . <eos>',\n",
              " '<sos> Five tens , and ten twenties , please . <eos>',\n",
              " '<sos> About 10 dollars . <eos>',\n",
              " '<sos> I want seven ten dollar bills and thirty one dollar bills . <eos>',\n",
              " '<sos> Two ten dollar bills , and two five dollar bills please . <eos>',\n",
              " '<sos> Make it 30 ten-dollar bills and small change . <eos>',\n",
              " '<sos> 8 ten-dollar bills and 4 five dollar bills , please . <eos>',\n",
              " '<sos> Fifty 10 dollar bills , please . <eos>',\n",
              " '<sos> Can I have change for this 10 dollar bill ? <eos>',\n",
              " '<sos> Could you give me small change for a ten dollar bill ? <eos>',\n",
              " '<sos> I will pay $ 10 in cash and the rest in credit card . <eos>',\n",
              " '<sos> 10dollars . <eos>',\n",
              " '<sos> Ten ten-dollar bills and ten one-dollar bills , please . <eos>',\n",
              " '<sos> Ten tens , ten fives , and fifty ones , please . <eos>',\n",
              " '<sos> 10 ten-dollar bills and 100 one-dollar bills , please . <eos>',\n",
              " '<sos> Twenty 10 dollar bills , forty 5 dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 's , two 5 's and ten 1 's . <eos>\",\n",
              " '<sos> Four tens , four fives and the rest in singles , please . <eos>',\n",
              " '<sos> 4 tens , and the rest in one dollar bills . <eos>',\n",
              " '<sos> 7 ten dollar bills and 6 five dollar bills , please . <eos>',\n",
              " '<sos> 2 tens and the rest in one dollar bills . <eos>',\n",
              " \"<sos> I 'd like three 10 dollar bills , and the rest in 1 dollar bills . <eos>\",\n",
              " '<sos> Ten tens , five twenties and coins for the rest , please . <eos>',\n",
              " \"<sos> I 'd like 10 ten-dollar bills , 20 five-dollar bills and 30 one-dollar bills . <eos>\",\n",
              " '<sos> 10dollar bills , please . <eos>',\n",
              " '<sos> Could you give me ten one-dollar bill for this ten dollar bill ? <eos>',\n",
              " '<sos> Thirty ten-dollar bills and small change for ten dollars . <eos>',\n",
              " '<sos> Please change this for ten dollar bills . <eos>',\n",
              " '<sos> Ten dollar bills , please . <eos>',\n",
              " '<sos> Would you like tens or twenties ? <eos>',\n",
              " '<sos> In tens and twenties . <eos>',\n",
              " '<sos> 5 ten dollar bills , 15 five-dollar bills and 20 singles . <eos>',\n",
              " '<sos> I need six tens and small change for the rest . <eos>',\n",
              " \"<sos> I 'd like to have 9 tens and the rest in fives . <eos>\",\n",
              " '<sos> Could you change this 10 dollar bill for me ? <eos>',\n",
              " \"<sos> I 'd like to have some tens and twenties . <eos>\",\n",
              " '<sos> In 10 and 20 dollar bills . <eos>',\n",
              " '<sos> Do you like it in dimes ? <eos>',\n",
              " '<sos> This is 10 kilograms overweight . <eos>',\n",
              " '<sos> This is 12 kilograms overweight . <eos>',\n",
              " '<sos> There are 10 kinds of them . <eos>',\n",
              " '<sos> To upgrade to the diamond level requires 100,000 points or more . <eos>',\n",
              " \"<sos> It 's Gate 10 . <eos>\",\n",
              " '<sos> Could you tell me the way to Gate 10 ? <eos>',\n",
              " '<sos> Please proceed to Gate 10 . <eos>',\n",
              " '<sos> It will depart in ten minutes . <eos>',\n",
              " '<sos> 10 minutes . Never mind . <eos>',\n",
              " '<sos> October 11th . <eos>',\n",
              " '<sos> We reserved a seat for October 25th , eleven a.m. <eos>',\n",
              " \"<sos> I 'd like to change the date to October 9th . <eos>\",\n",
              " \"<sos> I 'll be here for ten days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> For about ten days . <eos>',\n",
              " \"<sos> I 'll stay for 10 days . <eos>\",\n",
              " '<sos> For 10 days . <eos>',\n",
              " '<sos> 10 days . <eos>',\n",
              " '<sos> For ten days . <eos>',\n",
              " '<sos> We have 10 different brands . <eos>',\n",
              " '<sos> Where is Gate 11 ? <eos>',\n",
              " '<sos> Would you tell me how to get to Gate 11 ? <eos>',\n",
              " '<sos> It departs at 11:00 . <eos>',\n",
              " '<sos> How about the 11th ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> Where is seat 12B ? <eos>',\n",
              " '<sos> 12G is over there on the right side . <eos>',\n",
              " '<sos> Please go to Gate 12 . <eos>',\n",
              " '<sos> Please come to Gate 12 . <eos>',\n",
              " \"<sos> It 's time for boarding now at Gate 12 . <eos>\",\n",
              " '<sos> Boarding begins at 12:10 . <eos>',\n",
              " '<sos> The arrival time is twelve thirty local time . <eos>',\n",
              " \"<sos> Let me have two seats on the 12 o'clock flight . <eos>\",\n",
              " '<sos> I want to cancel my December 25 flight . <eos>',\n",
              " '<sos> I want to make it on December 27 . <eos>',\n",
              " '<sos> Please cancel my flight for December 4th . <eos>',\n",
              " \"<sos> I 'd like to change the date to December 7th . <eos>\",\n",
              " '<sos> Go to gate 13 , please . <eos>',\n",
              " \"<sos> There 's 13 hour 's difference . <eos>\",\n",
              " '<sos> One hundred forty dollars . Which seat do you prefer ? <eos>',\n",
              " \"<sos> There 's a 14 hour 's difference between two cities . Seoul is 14 hours ahead . <eos>\",\n",
              " '<sos> 15dollars , please . <eos>',\n",
              " '<sos> Where is seat 15A ? <eos>',\n",
              " '<sos> Your flight will depart from gate 15 . <eos>',\n",
              " \"<sos> There 's a 15 hour 's difference . Seoul is 14 hours ahead . <eos>\"]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_en_text_list[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLCS83YLL9lG",
        "outputId": "543a737b-14ba-44cb-a56f-cc3fb5724ecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<id 1>',\n",
              " '<sent 1>',\n",
              " '1\\t2\\tNP\\t777/SN',\n",
              " '2\\t6\\tNP_SBJ\\t항공편/NNG|은/JX',\n",
              " '3\\t4\\tNP\\t1/SN|시간/NNG',\n",
              " '4\\t6\\tNP_AJT\\t동안/NNG',\n",
              " '5\\t6\\tNP_AJT\\t지상/NNG|에/JKB',\n",
              " '6\\t7\\tVP\\t머물/VV|게/EC',\n",
              " '7\\t0\\tVP\\t되/VV|ㅂ니다/EF|./SF',\n",
              " '</sent>',\n",
              " '']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 한 문장씩 나눈 리스트 만들기\n",
        "ko_list = ko_lines.split(\"</id>\")\n",
        "for i in range(len(ko_list)):\n",
        "    ko_list[i] = ko_list[i].split(\"\\n\")\n",
        "\n",
        "ko_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilx4pFc-L-di"
      },
      "outputs": [],
      "source": [
        "# 필요한 정보만 추출\n",
        "import re\n",
        "\n",
        "pattern = r\"[가-힣ㄱ-ㅎ]+|[0-9]+(?=\\/SN)\"\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    for j in range(len(ko_list[i])):\n",
        "        ko_list[i][j] = re.findall(pattern, ko_list[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXDIcJuuL_XP",
        "outputId": "707b5784-75c2-43fc-f819-4e360fffc894"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[[],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['1', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['머물', '게'],\n",
              "  ['되', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['777'],\n",
              "  ['항공편', '은'],\n",
              "  ['3', '시간'],\n",
              "  ['동안'],\n",
              "  ['지상', '에'],\n",
              "  ['있', '겠', '습니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['1', '000', '달러'],\n",
              "  ['여행자', '수표', '가'],\n",
              "  ['필요', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['1', '250', '원', '이'], ['공식'], ['환율', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['3', '장', '과'],\n",
              "  ['나머지', '는'],\n",
              "  ['20', '달러', '권', '으로'],\n",
              "  ['주', '시', 'ㅂ시오'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러'],\n",
              "  ['한'],\n",
              "  ['장', '과'],\n",
              "  ['50', '달러'],\n",
              "  ['4', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '를'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '겠', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '만'], ['바꾸', '어'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전'],\n",
              "  ['좀'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '만'],\n",
              "  ['환전', '하', '아'],\n",
              "  ['어', '주', '어', '요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['6', '개'],\n",
              "  ['하', '고'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['8', '개', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '겠', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100', '불', '을'], ['바꾸', '겠', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['100', '불', '짜리'],\n",
              "  ['4', '매'],\n",
              "  ['20', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['3', '매'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['1', '매'],\n",
              "  ['그리고'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['5', '매'],\n",
              "  ['원하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['100'], ['마일리지', '가'], ['적립', '되', '었', '습니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['5', '장', '과'],\n",
              "  ['20', '달러'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['부탁', '하', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러'], ['정도', '이', '에요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['7', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['서른'],\n",
              "  ['장', '과'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐'],\n",
              "  ['여덟'],\n",
              "  ['장하', '고'],\n",
              "  ['5', '달러'],\n",
              "  ['지폐'],\n",
              "  ['네'],\n",
              "  ['장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '로'],\n",
              "  ['50', '장'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ'],\n",
              "  ['수'],\n",
              "  ['있', '습니까'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러'],\n",
              "  ['지폐', '를'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['바꾸', 'ㄹ', '려고요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '는'],\n",
              "  ['현금', '으로'],\n",
              "  ['하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['카드', '로'],\n",
              "  ['계산', '하', '아'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '이', 'ㅂ니다'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['10', '장'],\n",
              "  ['그리고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['50', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['10', '장', '하', '고'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['100', '장', '으로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['20', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['40', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['3', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['2', '장'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['10', '장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['4', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['부탁', '하', '아요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['4', '장', '하', '고'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['7', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['6', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['두'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['세'],\n",
              "  ['장이', '랑'],\n",
              "  ['나머지', '는'],\n",
              "  ['1', '달러', '로'],\n",
              "  ['주', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '과'],\n",
              "  ['20', '달러', '짜리'],\n",
              "  ['다섯'],\n",
              "  ['장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['동전', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['10', '장'],\n",
              "  ['5', '달러', '짜리'],\n",
              "  ['지폐'],\n",
              "  ['20', '장'],\n",
              "  ['1', '달러'],\n",
              "  ['지폐'],\n",
              "  ['30', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리'], ['지폐', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리'],\n",
              "  ['지폐', '를'],\n",
              "  ['1', '달러', '짜리'],\n",
              "  ['열'],\n",
              "  ['장', '으로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '가'],\n",
              "  ['서른'],\n",
              "  ['10', '달러', '는'],\n",
              "  ['잔돈', '이', 'ㅂ니다'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['바꾸', '어'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[], [], [], [], ['10', '달러', '짜리', '로'], ['주', '시', '어요'], [], []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  ['아니', '면'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['하', 'ㄹ까요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '달러', '짜리', '와'],\n",
              "  ['20', '달러', '짜리', '로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['5', '장'],\n",
              "  ['5', '불', '짜리'],\n",
              "  ['15', '장'],\n",
              "  ['1', '불', '짜리'],\n",
              "  ['20', '장', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []],\n",
              " [[],\n",
              "  [],\n",
              "  [],\n",
              "  [],\n",
              "  ['10', '불', '짜리'],\n",
              "  ['6', '장'],\n",
              "  ['그리고'],\n",
              "  ['나머지', '는'],\n",
              "  ['잔돈', '으로'],\n",
              "  ['주', '시', '어요'],\n",
              "  [],\n",
              "  []]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_list[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THlM-xVRMAaJ"
      },
      "outputs": [],
      "source": [
        "# 문장으로 만들어서 리스트에 넣어주기, <sos>, <eos> 토큰 각 문장에 넣어주기\n",
        "ko_text_list = []\n",
        "\n",
        "for i in range(len(ko_list)):\n",
        "    full_sent = '<sos>' + ' '\n",
        "    for j in range(len(ko_list[i])):\n",
        "        if ko_list[i][j]:\n",
        "            for token in ko_list[i][j]:\n",
        "                full_sent += token + ' '\n",
        "    full_sent += '<eos>'\n",
        "\n",
        "    ko_text_list.append(full_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxifIuNxMBR2",
        "outputId": "005a4431-2add-4b3d-82da-ef397bf95237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>',\n",
              " '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>',\n",
              " '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 만 환전 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 100 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 100 달러 짜리 4 장 20 달러 짜리 2 장 10 달러 짜리 5 장 1 달러 짜리 10 장 으로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 6 개 하 고 5 달러 짜리 지폐 8 개 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 100 달러 짜리 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 겠 습니까 <eos>',\n",
              " '<sos> 100 불 을 바꾸 겠 습니다 <eos>',\n",
              " '<sos> 100 불 짜리 4 매 20 불 짜리 3 매 10 불 짜리 3 매 5 불 짜리 1 매 그리고 1 불 짜리 5 매 원하 ㅂ니다 <eos>',\n",
              " '<sos> 100 마일리지 가 적립 되 었 습니다 <eos>',\n",
              " '<sos> 10 달러 5 장 과 20 달러 10 장 으로 부탁 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 정도 이 에요 <eos>',\n",
              " '<sos> 10 달러 지폐 7 장 1 달러 지폐 30 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 두 장 5 달러 짜리 두 장 주 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 서른 장 과 잔돈 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 여덟 장하 고 5 달러 지폐 네 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 지폐 로 50 장 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 수 있 습니까 <eos>',\n",
              " '<sos> 10 달러 지폐 를 잔돈 으로 바꾸 ㄹ 려고요 <eos>',\n",
              " '<sos> 10 달러 는 현금 으로 하 고 나머지 는 카드 로 계산 하 아 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 5 달러 짜리 10 장 그리고 1 달러 짜리 50 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 10 장 하 고 1 달러 짜리 100 장 으로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 20 장 5 달러 짜리 40 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 3 장 5 달러 짜리 2 장 1 달러 짜리 10 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 5 달러 짜리 4 장 그리고 나머지 는 1 달러 짜리 로 부탁 하 아요 <eos>',\n",
              " '<sos> 10 달러 짜리 4 장 하 고 나머지 는 1 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 7 장 5 달러 짜리 6 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 두 장이 랑 나머지 는 1 달러 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 세 장이 랑 나머지 는 1 달러 로 주 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 열 장 과 20 달러 짜리 다섯 장 그리고 나머지 는 동전 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 10 장 5 달러 짜리 지폐 20 장 1 달러 지폐 30 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 지폐 를 1 달러 짜리 열 장 으로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 가 서른 10 달러 는 잔돈 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 달러 짜리 로 바꾸 어 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 달러 짜리 로 하 ㄹ까요 아니 면 20 달러 짜리 로 하 ㄹ까요 <eos>',\n",
              " '<sos> 10 달러 짜리 와 20 달러 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 5 장 5 불 짜리 15 장 1 불 짜리 20 장 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 6 장 그리고 나머지 는 잔돈 으로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 9 장 나머지 는 5 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 짜리 를 바꾸 어 주 시 겠 어요 <eos>',\n",
              " '<sos> 10 불 짜리 와 20 불 짜리 로 주 시 어요 <eos>',\n",
              " '<sos> 10 불 하 고 20 불 짜리로요 <eos>',\n",
              " '<sos> 10 센트 로 드리 ㄹ 까요 <eos>',\n",
              " '<sos> 10 무게 초과 되 시 었 습니다 <eos>',\n",
              " '<sos> 12 초과 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 가지 종류 가 있 는데요 <eos>',\n",
              " '<sos> 10 만 점 이상 부터 다이아몬드 등급 으로 상향 조정 되 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 게이트 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 번 출구 로 가 는 길 을 알리 러 주 시 겠 습니까 <eos>',\n",
              " '<sos> 10 번 출구 로 가 시 어요 <eos>',\n",
              " '<sos> 10 분 후 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 10 분 이 ㅂ니다 신경 쓰 지 말 시 어요 <eos>',\n",
              " '<sos> 10 월 11 일 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 월 25 일 오전 11 시 발 비행 편 에 예약 되 었 습니다 <eos>',\n",
              " '<sos> 10 월 9 일 로 변경 하 고 싶 은데요 <eos>',\n",
              " '<sos> 10 일 동안 있 을 거 이 에요 <eos>',\n",
              " '<sos> 열흘 동안 이 요 <eos>',\n",
              " '<sos> 십 일 정도 <eos>',\n",
              " '<sos> 열흘 간 머무르 ㄹ 려구요 <eos>',\n",
              " '<sos> 십 일 간 이 ㅂ니다 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 일 이 요 <eos>',\n",
              " '<sos> 10 종류 의 상표 가 있 어요 <eos>',\n",
              " '<sos> 11 번 게이트 는 어디 에 있 나요 <eos>',\n",
              " '<sos> 11 번 게이트 에 어떻 게 가 는지 알리 러 주 시 겠 어요 <eos>',\n",
              " '<sos> 11 시 에 출발 하 ㅂ니다 <eos>',\n",
              " '<sos> 11 일 은 어떻 어요 <eos>',\n",
              " '<sos> 12 자리 가 어디 죠 <eos>',\n",
              " '<sos> 12 좌석 은 어디 이 ㅂ니까 <eos>',\n",
              " '<sos> 12 좌석 은 저기 오른쪽 에 있 습니다 <eos>',\n",
              " '<sos> 12 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 12 번 게이트 로 오 시 어요 <eos>',\n",
              " '<sos> 12 번 창구 에서 지금 탑승 하 시 ㅂ시오 <eos>',\n",
              " '<sos> 12 시 10 분 에 탑승 을 시작 하 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 30 분 에 도착 예정 이 ㅂ니다 <eos>',\n",
              " '<sos> 12 시 편 두 장 주 시 어요 <eos>',\n",
              " '<sos> 12 월 25 일 편 을 취소 하 고 싶 은데요 <eos>',\n",
              " '<sos> 12 월 27 일 로 하 아 주 시 어요 <eos>',\n",
              " '<sos> 12 월 4 일 예약 취소 하 아 어 주 어 요 <eos>',\n",
              " '<sos> 12 월 7 일 로 날짜 를 바꾸 고 싶 은데요 <eos>',\n",
              " '<sos> 13 번 게이트 로 가 시 어요 <eos>',\n",
              " '<sos> 13 시간 의 시차 가 있 습니다 <eos>',\n",
              " '<sos> 140 달러 이 ㅂ니다 어느 쪽 좌석 을 원하 시 어요 <eos>',\n",
              " '<sos> 14 시간 의 시차 가 있 습니다 서울 이 14 시간 빠르 죠 <eos>',\n",
              " '<sos> 15 달러 이 ㅂ니다 <eos>',\n",
              " '<sos> 15 좌석 은 어디 이 에요 <eos>',\n",
              " '<sos> 15 번 탑승구 는 번 중앙 홀 에 있 습니다 <eos>',\n",
              " '<sos> 15 시간 이 ㅂ니다 서울 이 14 시간 빠릅 니다 <eos>']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ko_text_list[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPpr-kfRMCuB"
      },
      "source": [
        "## 한-영 병행 데이터 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6xYAA8hMBYR",
        "outputId": "d617c8a6-ed66-4d91-98a0-7fe4151747e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<sos> Flight 007 will stay on the ground for one hour . <eos>', '<sos> Flight 017 will stay on the ground for three hours . <eos>', \"<sos> I need 1,000 dollars in traveler 's checks . <eos>\", '<sos> The official exchange rate is around 1,250 Won . <eos>', '<sos> Please give me three hundred dollar bills and twenty dollar bills for the rest . <eos>', '<sos> Can I have one hundred dollar bill and four fifty dollar bills ? <eos>', '<sos> Do you have change for $ 100 ? <eos>', \"<sos> I 'd like to change 100 dollars . <eos>\", \"<sos> I 'd like to change $ 100 . <eos>\", '<sos> Change 100 dollars . <eos>']\n",
            "['<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>', '<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>', '<sos> 1 000 달러 여행자 수표 가 필요 하 ㅂ니다 <eos>', '<sos> 1 250 원 이 공식 환율 이 ㅂ니다 <eos>', '<sos> 100 달러 3 장 과 나머지 는 20 달러 권 으로 주 시 ㅂ시오 <eos>', '<sos> 100 달러 한 장 과 50 달러 4 장 으로 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 를 바꾸 어 주 시 겠 어요 <eos>', '<sos> 100 달러 만 바꾸 어 주 시 어요 <eos>', '<sos> 100 달러 만 환전 좀 하 아 주 시 어요 <eos>', '<sos> 100 달러 만 환전 하 아 주 시 어요 <eos>']\n"
          ]
        }
      ],
      "source": [
        "print(full_en_text_list[:10])\n",
        "print(ko_text_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SbtRPSgMF9l"
      },
      "outputs": [],
      "source": [
        "parallel_data = list(zip(ko_text_list, full_en_text_list)) # src:한국어, trg:영어 순으로 넣어줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO28q99MMHNY",
        "outputId": "340aebda-def4-4581-8444-e8df853cabb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 1 시간 동안 지상 에 머물 게 되 ㅂ니다 <eos>',\n",
              " '<sos> Flight 007 will stay on the ground for one hour . <eos>')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQEA7EIYMH3Y",
        "outputId": "5f1e0c64-cfef-42d9-b9eb-5c53ebd7b31e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<sos> 777 항공편 은 3 시간 동안 지상 에 있 겠 습니다 <eos>',\n",
              " '<sos> Flight 017 will stay on the ground for three hours . <eos>')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enk37dBMWp6Q"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li6x5DU_NEw6",
        "outputId": "cd3443a6-a985-4b87-e450-7d6e825f0020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "import portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V31u4VrhLmaQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OixHsGQtLmaS",
        "outputId": "9c1bcf58-64ef-4012-8b05-47a170a5ec8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.15.2+cpu'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va5ZHr7-NJuB"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0go_LaxNOkZ"
      },
      "outputs": [],
      "source": [
        "#8:1:1 로 train, valid, test를 나눔\n",
        "\n",
        "train_iter = parallel_data[:int(len(parallel_data)*0.8)]\n",
        "valid_iter = parallel_data[int(len(parallel_data)*0.8):int(len(parallel_data)*0.9)]\n",
        "test_iter = parallel_data[int(len(parallel_data)*0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XVLG8AUNUO9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "valid_dataset = to_map_style_dataset(valid_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8QgJiXlNZUn"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(file_iter, type = 0):\n",
        "     \n",
        "     if type == 0:\n",
        "        for line in file_iter:\n",
        "            yield line[0].split()\n",
        "     else:\n",
        "        for line in file_iter:\n",
        "            yield line[1].split()\n",
        "\n",
        "src_voc = build_vocab_from_iterator(yield_tokens(train_iter, 0), specials=[\"<unk>\", \"<pad>\"])\n",
        "trg_voc = build_vocab_from_iterator(yield_tokens(train_iter, 1), specials=[\"<unk>\", \"<pad>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GQDJEP2NcNK",
        "outputId": "667cc09e-7b37-4e57-f610-07da530274ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29522\n",
            "33544\n"
          ]
        }
      ],
      "source": [
        "print(len(src_voc))\n",
        "print(len(trg_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvk9k4CoNbNA"
      },
      "outputs": [],
      "source": [
        "src_voc.set_default_index(src_voc[\"<unk>\"])\n",
        "trg_voc.set_default_index(trg_voc[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP4ugaIwNdUJ"
      },
      "outputs": [],
      "source": [
        "src_pipeline = lambda x: src_voc(x.split())\n",
        "trg_pipeline = lambda x: trg_voc(x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB0plOCPNe_8"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qso_PENoNgHg"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    src_list, trg_list, src_len = [], [], []\n",
        "    for (_src, _trg) in batch:\n",
        "        processed_src = torch.tensor(src_pipeline(_src), dtype=torch.int64)\n",
        "        src_list.append(processed_src)\n",
        "        src_len.append(len(processed_src))\n",
        "        processed_trg = torch.tensor(trg_pipeline(_trg), dtype=torch.int64)\n",
        "        trg_list.append(processed_trg)\n",
        "    \n",
        "    src_list = pad_sequence(src_list, padding_value=1, batch_first=True)\n",
        "    trg_list = pad_sequence(trg_list, padding_value=1, batch_first=True)\n",
        "    \n",
        "    return src_list.to(device), trg_list.to(device), src_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxDgEbreNhJl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# DataLoader를 사용하여 데이터셋을 배치 단위로 로드\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQO0m9DlOh1K"
      },
      "source": [
        "데이터셋이 배치 단위로 올바르게 훈련이 되는 지 뽑아보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c_zBnEyxwCN",
        "outputId": "27c95269-1369-4391-dd66-a3405d1c2e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   3,   89,  941,  ...,    1,    1,    1],\n",
            "        [   3,  470,  411,  ...,    1,    1,    1],\n",
            "        [   3,   65,   72,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   3,  715,    4,  ...,    1,    1,    1],\n",
            "        [   3, 5348,   56,  ...,    1,    1,    1],\n",
            "        [   3,   39,    8,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[   3,    6,  265,  ...,    1,    1,    1],\n",
            "        [   3,   60,    6,  ...,    1,    1,    1],\n",
            "        [   3,  373,   17,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   3,   40,   16,  ...,    1,    1,    1],\n",
            "        [   3,   42, 3282,  ...,    1,    1,    1],\n",
            "        [   3,   81, 2481,  ...,    1,    1,    1]], device='cuda:0')\n",
            "128\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "# 대표로 train_dataloader 첫 번째 배치를 가져오기\n",
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "print(batch[0]) #src\n",
        "print(batch[1]) #trg\n",
        "print(len(batch[0])) #src 개수, 128개여야 함\n",
        "print(len(batch[1])) #trg 개수, 128개여야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KM0R_LZOyJf"
      },
      "source": [
        " batch 형태가 정상적이며, 데이터셋이 배치 단위로 올바르게 훈련이 되어야합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac2X4bDuN_8M"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKz-5pzJLmaf"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTnTagvULmai"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWbUIJ7QvjVd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#overfitting을 의심해보고 multi-head attention layer에서 배치 정규화를 시도\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "        self.batch_norm = nn.BatchNorm1d(hid_dim)  # 배치 정규화\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "        \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        # 배치 정규화를 적용\n",
        "        x_norm = self.batch_norm(x.permute(0, 2, 1))\n",
        "        x_norm = x_norm.permute(0, 2, 1)\n",
        "        \n",
        "        x = self.fc_o(x_norm)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y20_Q7jWLmam"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGoHoWTNLmap"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFDkHy4oLmaq"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcPjHYrMLmas"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dQ60nFvO-Cp"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(src_voc)\n",
        "OUTPUT_DIM = len(trg_voc)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.25 #train/valid/test/inference는 성능이 좋은데 이상하게 bleu score만 성능이 낮아 overfitting을 의심해보고 해결 방안으로 dropout 비율을 0.1에서 0.25로 늘려봄\n",
        "DEC_DROPOUT = 0.25\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLagOl3eLmat"
      },
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = 1\n",
        "TRG_PAD_IDX = 1\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPTuMMlfLmau",
        "outputId": "01b61bc1-fd09-4fc7-b63b-19afeed97c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 28,819,213 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLuxYlCeLmav"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H79Xs56OLmaw"
      },
      "outputs": [],
      "source": [
        "model.apply(initialize_weights);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hABmJ-f2Lma9"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phIXiWQsLma-"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZzikLDCLma_"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch[0]\n",
        "        trg = batch[1]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBQQHUbrLmbB"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpTIAHSiLmbC"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJYccsic6PZE"
      },
      "source": [
        "훈련 과정에서 validation set에 대한 loss 수렴이 잘 이뤄지고 있는지 판단하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0yBGC3yLmbD",
        "outputId": "1a6a634e-3b8b-4482-c432-0726ac8659e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 4m 21s\n",
            "\tTrain Loss: 3.030 | Train PPL:  20.695\n",
            "\t Val. Loss: 2.555 |  Val. PPL:  12.867\n",
            "Epoch: 02 | Time: 4m 16s\n",
            "\tTrain Loss: 1.899 | Train PPL:   6.681\n",
            "\t Val. Loss: 2.000 |  Val. PPL:   7.391\n",
            "Epoch: 03 | Time: 4m 17s\n",
            "\tTrain Loss: 1.546 | Train PPL:   4.695\n",
            "\t Val. Loss: 1.701 |  Val. PPL:   5.479\n",
            "Epoch: 04 | Time: 4m 17s\n",
            "\tTrain Loss: 1.351 | Train PPL:   3.862\n",
            "\t Val. Loss: 1.498 |  Val. PPL:   4.471\n",
            "Epoch: 05 | Time: 4m 16s\n",
            "\tTrain Loss: 1.216 | Train PPL:   3.373\n",
            "\t Val. Loss: 1.353 |  Val. PPL:   3.869\n",
            "Epoch: 06 | Time: 4m 16s\n",
            "\tTrain Loss: 1.115 | Train PPL:   3.050\n",
            "\t Val. Loss: 1.238 |  Val. PPL:   3.449\n",
            "Epoch: 07 | Time: 4m 16s\n",
            "\tTrain Loss: 1.035 | Train PPL:   2.814\n",
            "\t Val. Loss: 1.158 |  Val. PPL:   3.184\n",
            "Epoch: 08 | Time: 4m 17s\n",
            "\tTrain Loss: 0.971 | Train PPL:   2.642\n",
            "\t Val. Loss: 1.088 |  Val. PPL:   2.968\n",
            "Epoch: 09 | Time: 4m 16s\n",
            "\tTrain Loss: 0.919 | Train PPL:   2.506\n",
            "\t Val. Loss: 1.033 |  Val. PPL:   2.808\n",
            "Epoch: 10 | Time: 4m 16s\n",
            "\tTrain Loss: 0.876 | Train PPL:   2.400\n",
            "\t Val. Loss: 0.982 |  Val. PPL:   2.671\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qAME1RN0gzV"
      },
      "source": [
        "훈련 과정에서 validation set에 대한 loss 수렴이 잘 이뤄지고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj9624OhOZd0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYFmm6gILmbE",
        "outputId": "831b0b93-6598-463e-b3c0-083685ae65a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 0.907 | Test PPL:   2.476 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGc5h0kmLmbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaIne_kDTGJS"
      },
      "outputs": [],
      "source": [
        "trg_itos = trg_voc.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZJjpnZLLmbG"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_voc, trg_voc, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token for token in sentence.split()]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "        \n",
        "    src_indexes = [src_voc[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor) #transformer 모델에서는 src에 mask 적용\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_voc['<sos>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor) #trg에도 mask 적용\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "                \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_voc['<eos>']:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8UDti_3QBQb"
      },
      "outputs": [],
      "source": [
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii5vw18zFIIc",
        "outputId": "c391c951-d444-4123-ca96-ba0567114bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .\n",
            "['All', 'liquids', ',', 'gels', 'and', 'aerosols', 'must', 'be', 'placed', 'in', 'a', 'single', 'and', 'single', 'and', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'single', 'and', 'a', 'single', 'and']\n",
            "미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?\n",
            "['I', \"'m\", 'sorry', ',', 'but', 'I', \"'d\", 'like', 'to', 'change', 'my', 'ticket', 'to', 'the', 'child', '.', 'Could', 'you', 'give', 'me', 'a', 'ticket', 'to', 'the', 'double', 'decker', 'bus', 'number', 'is', 'a', 'ticket', 'to', 'the', 'movie', 'ticket', ',', 'please', 'change', 'for', 'me', 'a', 'ticket', 'to', 'the', 'movie', 'beer', ',', 'and', 'reserve']\n",
            "은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요\n",
            "['The', 'bank', 'is', 'too', 'far', '.', 'Do', 'you', 'want', 'to', 'pay', 'too', 'much', 'cash', 'or', 'so', 'much', 'money', '?']\n",
            "아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?\n",
            "['I', \"'m\", 'afraid', 'I', \"'ve\", 'lost', 'my', 'baggage', 'claim', '.', 'Do', 'you', 'want', 'to', 'fill', 'out', 'the', 'lost', 'our', 'office', 'to', 'fill', 'out', 'the', 'immigration', 'form', '?']\n",
            "부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .\n",
            "['The', 'Busan', 'of', 'the', 'strict', 'east', 'bound', 'for', 'Busan', ',', 'so', 'I', \"'m\", 'going', 'to', 'take', 'from', 'the', 'northern', 'and', 'collected', 'at', 'the', 'northern', 'and', 'collected', 'in', 'the', 'northern', 'and', 'so', 'far', 'east', 'school', '.']\n",
            "변기 가 막히 었 습니다 .\n",
            "['The', 'toilet', 'is', 'clogged', 'up', '.']\n",
            "그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?\n",
            "['I', \"'d\", 'like', 'to', 'buy', 'some', 'pants', 'which', 'I', 'bought', 'this', 'pants', 'for', 'trousers', '.', 'How', 'much', 'is', 'it', 'for', '?']\n",
            "비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .\n",
            "['I', 'think', 'it', 'rains', 'at', 'the', 'department', 'store', 'and', 'I', \"'d\", 'rather', 'go', 'to', 'the', 'dark', '.']\n",
            "속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다\n",
            "['My', 'health', 'is', 'uncomfortable', 'even', 'in', 'the', 'soup', '.']\n",
            "문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .\n",
            "['The', 'President', 'said', 'that', 'the', 'group', 'is', 'not', 'beneficial', 'from', 'its', 'own', 'private', 'documents', '.']\n",
            "이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .\n",
            "['How', 'many', 'hours', 'do', 'you', 'take', 'for', 'this', '?']\n",
            "이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .\n",
            "['This', 'ship', 'was', 'accepted', 'on', 'the', 'Oeinmyoji', '.']\n",
            "통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .\n",
            "['The', 'story', 'is', 'as', 'shown', 'as', 'the', 'worst', 'terror', 'act', 'as', 'follows', ';']\n",
            "이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .\n",
            "['I', 'think', 'it', 'will', 'be', 'a', 'great', 'market', 'to', 'develop', 'our', 'products', 'effectively', '.']\n",
            "요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 \n",
            "['I', 'usually', 'read', 'the', 'program', 'before', 'these', 'days', '.']\n"
          ]
        }
      ],
      "source": [
        "for sent in sen_list:\n",
        "  print(sent)\n",
        "  print(translate_sentence(sent, src_voc, trg_voc, model, device, max_len = 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03qgETyOLmbV"
      },
      "source": [
        "## BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtzDsinh6UTJ"
      },
      "source": [
        "모든 훈련 스텝에 문제가 없다면 bleu score를 측정하는 내부 코드를 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQwSjXBNLmbV"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data, src_voc, trg_voc, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data: #test_dataset의 원소 하나하나에 대해\n",
        "        \n",
        "        src = datum[0]\n",
        "        trg = datum[1].split() #trg는 문장 형태로 되어있으므로 단어별로 토큰화하여 하나의 리스트로 만들기, split()을 이용\n",
        "\n",
        "        trg.pop(0) #<sos> 제거\n",
        "        trg.pop() #<eos> 제거\n",
        "        \n",
        "        pred_trg = translate_sentence(src, src_voc, trg_voc, model, device, max_len)\n",
        "        #위의 inference 형태처럼 pred_trg는 단어별로 토큰화되어 하나의 리스트 안에 들어가 있음\n",
        "\n",
        "        #단어별로 토큰화된 리스트를 pred_trgs, trgs에 넣어줌\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append(trg)\n",
        "        \n",
        "    #단어별로 토큰화된 리스트'들' 전체(pred_trgs, trgs)를 대상으로 corpus_bleu 메서드를 이용해 bleu score를 구함.\n",
        "    return corpus_bleu(pred_trgs, trgs, weights=(1, 0, 0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8GAJOMy2IBR"
      },
      "source": [
        "calculate_bleu 함수 내부 코드를 확인하였으며, 이는 학우들과 상의하며 수정한 코드로, 똑같은 코드에 대해서 학우들은 정상적으로 bleu score가 산출되었기에 bleu 함수에는 문제가 없다고 판단했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Fq8g56LmbW",
        "outputId": "38c83d35-cc5c-47ab-d047-962bbafae7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score = 18.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "bleu_score = calculate_bleu(test_dataset, src_voc, trg_voc, model, device, max_len=50)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}') #train/valid/test/inference는 성능이 좋은데 이상하게 bleu score만 성능이 낮음. overfitting을 의심해보고 배치 정규화/dropout 비율 조정 등 다양한 시도를 해보았으나 안타깝게 해결되지 않음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTA6EIE06ls_"
      },
      "source": [
        "훈련 스텝과 bleu score 내부 코드를 확인해보았으나 특이하게 transformer에 대해서만 bleu score가 낮게 나옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eqISm5d2XUV"
      },
      "source": [
        "# 결론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnxCUWvY2iF2"
      },
      "source": [
        "## Packed Encoder-Decoder 모델\n",
        "- 가장 느렸다.\n",
        "- Bleu score 37.47이다.\n",
        "- Inference는 그렇게 잘 반영되는 것 같지는 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2ykNOV3B-6"
      },
      "source": [
        "## Convolutional Seq to Seq 모델\n",
        "- Packed Encoder-Decoder 모델보다는 빨랐지만, Transformers 모델보다는 느렸다.\n",
        "- Bleu score 48.12로 가장 높았다.\n",
        "- Inference는 어느 정도 괜찮게 나오는 것으로 보인다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1AAHvfS3VHX"
      },
      "source": [
        "## Transformers 모델\n",
        "- 가장 빨랐다. (Convolutional Seq to Seq 모델보다 약 4배 정도 빨랐다.)\n",
        "- Bleu score 18.69로 예상보다 낮게 나왔다. (훈련 스텝과 bleu score 내부 코드를 확인해보았으나 특이하게 transformer에 대해서만 bleu score가 낮게 나온다.)\n",
        "- 하지만 Inference는 잘 나오는 것으로 보이고, loss, perplexity도 전체적으로 가장 낮게 나온 것으로 보아 학습은 제대로 이루어진 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNcJ4zkkyuN"
      },
      "source": [
        "## 요약"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngg_t-iGk7iN"
      },
      "source": [
        "Bleu score가 가장 높게 나온 모델은 CNN seq to seq 모델이다. 이 모델의 inference를 다시 한 번 보여주자면 아래와 같다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP9gbR6ulDwd"
      },
      "outputs": [],
      "source": [
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHtzanfYlDwk",
        "outputId": "818ab9dc-4749-4a54-9006-f6087a344588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted trg = ['I', 'have', 'to', 'put', 'them', 'of', 'liquids', ',', 'and', 'aerosols', ',', 'and', 'aerosols', ',', 'and', 'aerosols', ',', 'and', 'a', 'plastic', 'bag', '.']\n",
            "predicted trg = ['I', \"'m\", 'sorry', ',', 'but', 'I', \"'d\", 'like', 'to', 'have', 'a', 'little', 'meeting', 'of', 'the', 'speed', '.', 'May', 'I', 'have', 'a', 'ticket', 'for', 'a', 'few', 'distance', '?']\n",
            "predicted trg = ['The', 'bank', 'is', 'too', 'far', 'from', 'the', 'bank', '.', 'I', 'need', 'a', 'little', '.', 'I', 'need', 'to', 'be', 'a', 'little', '.']\n",
            "predicted trg = ['I', 'think', 'I', 'have', 'lost', 'lost', 'the', 'lost', ',', 'but', 'I', 'have', 'to', 'have', 'a', 'lost', 'of', 'the', 'lost', '.', 'Do', 'you', 'want', 'to', 'have', 'a', 'lost', 'of', 'the', 'lost', '?', 'I', 'need', 'to', 'be', 'out', 'with', 'the', 'lost', '.', 'Do', 'you', 'have', 'to', 'put', 'the', 'lost', 'lost', 'or', 'I']\n",
            "predicted trg = ['I', \"'m\", 'going', 'to', 'get', 'a', 'few', 'minute', 'call', 'from', 'Busan', ',', 'I', 'can', 'Daegu', 'a', 'few', 'minute', '.']\n",
            "predicted trg = ['The', 'toilet', 'was', 'blocked', '.', 'It', 'was', 'caught', '.']\n",
            "predicted trg = ['Please', 'show', 'me', 'that', 'pants', ',', 'please', '.', 'How', 'much', 'can', 'I', 'buy', 'it', '?']\n",
            "predicted trg = ['I', \"'d\", 'like', 'to', 'go', 'to', 'Macy', 'on', 'the', 'department', 'department', '.', 'I', \"'d\", 'like', 'to', 'go', 'to', 'Duta', ',', 'I', \"'d\", 'like', 'to', 'go', 'to', 'the', 'Duta', '.']\n",
            "predicted trg = ['I', 'do', \"n't\", 'feel', 'like', 'to', 'have', 'a', 'good', 'breakfast', ',', 'but', 'I', 'do', \"n't\", 'have', 'to', 'be', 'able', 'to', 'have', 'a', 'good', 'breakfast', '.']\n",
            "predicted trg = ['The', 'door', 'President', 'was', 'ready', 'to', 'get', 'a', 'little', 'profit', '.', 'I', 'told', 'you', '.']\n",
            "predicted trg = ['I', \"'d\", 'like', 'to', 'have', 'some', 'time', 'to', 'see', 'this', '.']\n",
            "predicted trg = ['The', 'average', 'was', 'all', 'a', 'little', 'bit', 'of', 'Oeinmyoji', 'of', 'Oeinmyoji', 'of', 'Oeinmyoji', '.']\n",
            "predicted trg = ['I', \"'ve\", 'been', 'sorry', 'of', 'the', 'average', 'of', 'the', 'delay', 'of', 'the', 'delay', 'of', 'the', 'delay', '.']\n",
            "predicted trg = ['I', 'think', 'this', 'product', 'is', 'made', 'to', 'be', 'a', 'little', 'of', 'the', 'company', 'of', 'the', 'sales', 'of', 'the', 'company', ',', 'and', 'I', \"'ll\", 'understand', 'you', '.']\n",
            "predicted trg = ['Nowadays', 'is', 'a', 'program', 'of', 'the', 'program', 'of', 'the', 'program', 'of', 'the', 'program', '.']\n"
          ]
        }
      ],
      "source": [
        "for sent in sen_list:\n",
        "    translation, attention = translate_sentence(sent, src_voc, trg_voc, model, device)\n",
        "    print(f'predicted trg = {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVXqdTFvlFoU"
      },
      "source": [
        "전체적인 Loss와 Perplexity가 가장 낮게 나온 모델은 Transformers 모델이다. 이 모델의 inference를 다시 한 번 보여주자면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ_uoplDlOE4"
      },
      "outputs": [],
      "source": [
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-RiwQ34lOE_",
        "outputId": "c391c951-d444-4123-ca96-ba0567114bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .\n",
            "['All', 'liquids', ',', 'gels', 'and', 'aerosols', 'must', 'be', 'placed', 'in', 'a', 'single', 'and', 'single', 'and', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'a', 'single', 'and', 'single', 'and', 'a', 'single', 'and']\n",
            "미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?\n",
            "['I', \"'m\", 'sorry', ',', 'but', 'I', \"'d\", 'like', 'to', 'change', 'my', 'ticket', 'to', 'the', 'child', '.', 'Could', 'you', 'give', 'me', 'a', 'ticket', 'to', 'the', 'double', 'decker', 'bus', 'number', 'is', 'a', 'ticket', 'to', 'the', 'movie', 'ticket', ',', 'please', 'change', 'for', 'me', 'a', 'ticket', 'to', 'the', 'movie', 'beer', ',', 'and', 'reserve']\n",
            "은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요\n",
            "['The', 'bank', 'is', 'too', 'far', '.', 'Do', 'you', 'want', 'to', 'pay', 'too', 'much', 'cash', 'or', 'so', 'much', 'money', '?']\n",
            "아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?\n",
            "['I', \"'m\", 'afraid', 'I', \"'ve\", 'lost', 'my', 'baggage', 'claim', '.', 'Do', 'you', 'want', 'to', 'fill', 'out', 'the', 'lost', 'our', 'office', 'to', 'fill', 'out', 'the', 'immigration', 'form', '?']\n",
            "부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .\n",
            "['The', 'Busan', 'of', 'the', 'strict', 'east', 'bound', 'for', 'Busan', ',', 'so', 'I', \"'m\", 'going', 'to', 'take', 'from', 'the', 'northern', 'and', 'collected', 'at', 'the', 'northern', 'and', 'collected', 'in', 'the', 'northern', 'and', 'so', 'far', 'east', 'school', '.']\n",
            "변기 가 막히 었 습니다 .\n",
            "['The', 'toilet', 'is', 'clogged', 'up', '.']\n",
            "그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?\n",
            "['I', \"'d\", 'like', 'to', 'buy', 'some', 'pants', 'which', 'I', 'bought', 'this', 'pants', 'for', 'trousers', '.', 'How', 'much', 'is', 'it', 'for', '?']\n",
            "비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .\n",
            "['I', 'think', 'it', 'rains', 'at', 'the', 'department', 'store', 'and', 'I', \"'d\", 'rather', 'go', 'to', 'the', 'dark', '.']\n",
            "속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다\n",
            "['My', 'health', 'is', 'uncomfortable', 'even', 'in', 'the', 'soup', '.']\n",
            "문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .\n",
            "['The', 'President', 'said', 'that', 'the', 'group', 'is', 'not', 'beneficial', 'from', 'its', 'own', 'private', 'documents', '.']\n",
            "이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .\n",
            "['How', 'many', 'hours', 'do', 'you', 'take', 'for', 'this', '?']\n",
            "이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .\n",
            "['This', 'ship', 'was', 'accepted', 'on', 'the', 'Oeinmyoji', '.']\n",
            "통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .\n",
            "['The', 'story', 'is', 'as', 'shown', 'as', 'the', 'worst', 'terror', 'act', 'as', 'follows', ';']\n",
            "이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .\n",
            "['I', 'think', 'it', 'will', 'be', 'a', 'great', 'market', 'to', 'develop', 'our', 'products', 'effectively', '.']\n",
            "요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 \n",
            "['I', 'usually', 'read', 'the', 'program', 'before', 'these', 'days', '.']\n"
          ]
        }
      ],
      "source": [
        "for sent in sen_list:\n",
        "  print(sent)\n",
        "  print(translate_sentence(sent, src_voc, trg_voc, model, device, max_len = 50))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
